{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3dcb163-55e6-4a65-a845-2073b1b0d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9ac020-8001-4aac-a5f3-eddaeb6a00c3",
   "metadata": {},
   "source": [
    "### Importing saved data, analyzed on LSTM model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45ebd2f9-6082-44f1-bf77-97b2dbd5da37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('clean_df_MULTICLASS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97ad5801-838a-469f-892c-8f325100a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8750497a-3aae-492f-b941-d4dd5ab006bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CAT1</th>\n",
       "      <th>ORD1</th>\n",
       "      <th>ORD2</th>\n",
       "      <th>ORD3</th>\n",
       "      <th>ORD4</th>\n",
       "      <th>ORD5</th>\n",
       "      <th>ORD6</th>\n",
       "      <th>ORD7</th>\n",
       "      <th>ORD8</th>\n",
       "      <th>ORD9</th>\n",
       "      <th>ORD10</th>\n",
       "      <th>ORD11</th>\n",
       "      <th>ORD12</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.090</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.967</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.967</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.062</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.062</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>0</td>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.479</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>0</td>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.288</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>0</td>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.168</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>0</td>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.389</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>0</td>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.505</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1012 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CAT1     ORD1  ORD2   ORD3   ORD4   ORD5  ORD6   ORD7  ORD8  ORD9  \\\n",
       "0        0  0.00632  18.0   2.31  0.538  6.575  65.2  4.090     1   296   \n",
       "1        0  0.02731   0.0   7.07  0.469  6.421  78.9  4.967     2   242   \n",
       "2        0  0.02729   0.0   7.07  0.469  7.185  61.1  4.967     2   242   \n",
       "3        0  0.03237   0.0   2.18  0.458  6.998  45.8  6.062     3   222   \n",
       "4        0  0.06905   0.0   2.18  0.458  7.147  54.2  6.062     3   222   \n",
       "...    ...      ...   ...    ...    ...    ...   ...    ...   ...   ...   \n",
       "1007     0  0.06263   0.0  11.93  0.573  6.593  69.1  2.479     1   273   \n",
       "1008     0  0.04527   0.0  11.93  0.573  6.120  76.7  2.288     1   273   \n",
       "1009     0  0.06076   0.0  11.93  0.573  6.976  91.0  2.168     1   273   \n",
       "1010     0  0.10959   0.0  11.93  0.573  6.794  89.3  2.389     1   273   \n",
       "1011     0  0.04741   0.0  11.93  0.573  6.030  80.8  2.505     1   273   \n",
       "\n",
       "      ORD10   ORD11  ORD12  PRICE  \n",
       "0      15.3  396.90   4.98      2  \n",
       "1      17.8  396.90   9.14      1  \n",
       "2      17.8  392.83   4.03      2  \n",
       "3      18.7  394.63   2.94      2  \n",
       "4      18.7  396.90   5.33      2  \n",
       "...     ...     ...    ...    ...  \n",
       "1007   21.0  391.99   9.67      1  \n",
       "1008   21.0  396.90   9.08      1  \n",
       "1009   21.0  396.90   5.64      2  \n",
       "1010   21.0  393.45   6.48      1  \n",
       "1011   21.0  396.90   7.88      0  \n",
       "\n",
       "[1012 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccc601a-e010-48ad-a2cb-a34730c221cc",
   "metadata": {},
   "source": [
    "### Train,test,split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a004e06-885e-43f2-ad6a-b92568a96b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,0:13],df.iloc[:,-1],test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ccf8b06-cc81-4fe7-9df9-69b26584db23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((809, 13), (203, 13), (809,), (203,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9ccaea-9f11-4883-ac7d-bdc29410cc77",
   "metadata": {},
   "source": [
    "### Making k-1 dummy variable of 'ORD8' categorical column using OneHotEncoder since nominal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b476af9d-8790-4037-b7d3-628ad69c5249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "transformer = ColumnTransformer(transformers=[\n",
    "    ('tnf1' ,OneHotEncoder(sparse_output=False,drop='first'),['ORD8'])\n",
    "     ],remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c19c948-799a-4b56-99dd-57508cde4f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = transformer.fit_transform(X_train)\n",
    "X_test_new = transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61808728-1c59-48ca-9ad4-7cad98f5f685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((809, 20), (203, 20), (809,), (203,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new.shape, X_test_new.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f84555e-aa31-4a7b-a7b8-824059141cd5",
   "metadata": {},
   "source": [
    "### Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d5537bf-999c-4b7f-9124-80ae9b644c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_new)\n",
    "X_test_scaled = scaler.transform(X_test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea72667-4b99-4ff1-a98b-fb2af4d8dd16",
   "metadata": {},
   "source": [
    "### Reshape data for RNN [samples, timesteps, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ece0025-2086-4a2b-af86-00333634e980",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16231027-3901-4648-b528-98dca33f08f3",
   "metadata": {},
   "source": [
    "### RNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67586b62-3a62-47fa-8fb1-952013cf3f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(50, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "409e9280-b805-4cc5-92d6-59799a8c384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8315f77-a380-44a6-a192-2b435067022b",
   "metadata": {},
   "source": [
    "### callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56260680-4f3c-4c3f-92b0-54f6a96447ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "checkpoint = ModelCheckpoint('best_model_RNN_multiclass.h5', monitor='val_loss', save_best_only=True, mode='min',verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf74e1b8-eba6-419b-883a-9dbee64f5644",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "757b3bf6-ac63-4ffb-863a-a8d2572a9b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0038 - accuracy: 0.4745\n",
      "Epoch 1: val_loss improved from inf to 0.89775, saving model to best_model_RNN_multiclass.h5\n",
      "21/21 [==============================] - 5s 66ms/step - loss: 1.0038 - accuracy: 0.4745 - val_loss: 0.8977 - val_accuracy: 0.5617\n",
      "Epoch 2/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.8172 - accuracy: 0.6047\n",
      "Epoch 2: val_loss improved from 0.89775 to 0.78201, saving model to best_model_RNN_multiclass.h5\n",
      "21/21 [==============================] - 1s 36ms/step - loss: 0.8156 - accuracy: 0.6059 - val_loss: 0.7820 - val_accuracy: 0.5926\n",
      "Epoch 3/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.7366 - accuracy: 0.6562\n",
      "Epoch 3: val_loss improved from 0.78201 to 0.70800, saving model to best_model_RNN_multiclass.h5\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 0.7354 - accuracy: 0.6584 - val_loss: 0.7080 - val_accuracy: 0.7099\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6803 - accuracy: 0.7017\n",
      "Epoch 4: val_loss improved from 0.70800 to 0.67089, saving model to best_model_RNN_multiclass.h5\n",
      "21/21 [==============================] - 1s 37ms/step - loss: 0.6803 - accuracy: 0.7017 - val_loss: 0.6709 - val_accuracy: 0.7037\n",
      "Epoch 5/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.6351 - accuracy: 0.7000\n",
      "Epoch 5: val_loss improved from 0.67089 to 0.60305, saving model to best_model_RNN_multiclass.h5\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 0.6360 - accuracy: 0.7017 - val_loss: 0.6031 - val_accuracy: 0.7531\n",
      "Epoch 6/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.6036 - accuracy: 0.7434\n",
      "Epoch 6: val_loss did not improve from 0.60305\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.6074 - accuracy: 0.7388 - val_loss: 0.6149 - val_accuracy: 0.6914\n",
      "Epoch 7/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5963 - accuracy: 0.7344\n",
      "Epoch 7: val_loss improved from 0.60305 to 0.57559, saving model to best_model_RNN_multiclass.h5\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.5992 - accuracy: 0.7342 - val_loss: 0.5756 - val_accuracy: 0.7716\n",
      "Epoch 8/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5648 - accuracy: 0.7747\n",
      "Epoch 8: val_loss improved from 0.57559 to 0.50753, saving model to best_model_RNN_multiclass.h5\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 0.5662 - accuracy: 0.7682 - val_loss: 0.5075 - val_accuracy: 0.7778\n",
      "Epoch 9/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5487 - accuracy: 0.7697\n",
      "Epoch 9: val_loss improved from 0.50753 to 0.49887, saving model to best_model_RNN_multiclass.h5\n",
      "21/21 [==============================] - 1s 35ms/step - loss: 0.5510 - accuracy: 0.7713 - val_loss: 0.4989 - val_accuracy: 0.7654\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5179 - accuracy: 0.7991\n",
      "Epoch 10: val_loss did not improve from 0.49887\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.5179 - accuracy: 0.7991 - val_loss: 0.5283 - val_accuracy: 0.7531\n",
      "Epoch 11/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5072 - accuracy: 0.7944\n",
      "Epoch 11: val_loss improved from 0.49887 to 0.46982, saving model to best_model_RNN_multiclass.h5\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 0.4986 - accuracy: 0.7960 - val_loss: 0.4698 - val_accuracy: 0.7840\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5128 - accuracy: 0.7975\n",
      "Epoch 12: val_loss improved from 0.46982 to 0.45757, saving model to best_model_RNN_multiclass.h5\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.5128 - accuracy: 0.7975 - val_loss: 0.4576 - val_accuracy: 0.7963\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4636 - accuracy: 0.8161\n",
      "Epoch 13: val_loss did not improve from 0.45757\n",
      "21/21 [==============================] - 1s 37ms/step - loss: 0.4636 - accuracy: 0.8161 - val_loss: 0.4681 - val_accuracy: 0.7901\n",
      "Epoch 14/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4605 - accuracy: 0.8141\n",
      "Epoch 14: val_loss did not improve from 0.45757\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 0.4589 - accuracy: 0.8145 - val_loss: 0.4795 - val_accuracy: 0.7901\n",
      "Epoch 15/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4829 - accuracy: 0.8000\n",
      "Epoch 15: val_loss improved from 0.45757 to 0.42882, saving model to best_model_RNN_multiclass.h5\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 0.4807 - accuracy: 0.8006 - val_loss: 0.4288 - val_accuracy: 0.8148\n",
      "Epoch 16/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4496 - accuracy: 0.8062\n",
      "Epoch 16: val_loss did not improve from 0.42882\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.4461 - accuracy: 0.8083 - val_loss: 0.4318 - val_accuracy: 0.8086\n",
      "Epoch 17/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4590 - accuracy: 0.8130\n",
      "Epoch 17: val_loss improved from 0.42882 to 0.41518, saving model to best_model_RNN_multiclass.h5\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 0.4590 - accuracy: 0.8130 - val_loss: 0.4152 - val_accuracy: 0.8148\n",
      "Epoch 18/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4612 - accuracy: 0.8188\n",
      "Epoch 18: val_loss did not improve from 0.41518\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 0.4594 - accuracy: 0.8192 - val_loss: 0.4467 - val_accuracy: 0.8086\n",
      "Epoch 19/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4463 - accuracy: 0.8203\n",
      "Epoch 19: val_loss did not improve from 0.41518\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.4461 - accuracy: 0.8192 - val_loss: 0.4274 - val_accuracy: 0.8148\n",
      "Epoch 20/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4353 - accuracy: 0.8253\n",
      "Epoch 20: val_loss did not improve from 0.41518\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 0.4353 - accuracy: 0.8253 - val_loss: 0.4390 - val_accuracy: 0.8333\n",
      "Epoch 21/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4181 - accuracy: 0.8438\n",
      "Epoch 21: val_loss improved from 0.41518 to 0.40970, saving model to best_model_RNN_multiclass.h5\n",
      "21/21 [==============================] - 1s 38ms/step - loss: 0.4234 - accuracy: 0.8393 - val_loss: 0.4097 - val_accuracy: 0.8148\n",
      "Epoch 22/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4160 - accuracy: 0.8297\n",
      "Epoch 22: val_loss improved from 0.40970 to 0.40487, saving model to best_model_RNN_multiclass.h5\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.4153 - accuracy: 0.8315 - val_loss: 0.4049 - val_accuracy: 0.8580\n",
      "Epoch 23/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3989 - accuracy: 0.8391\n",
      "Epoch 23: val_loss improved from 0.40487 to 0.40455, saving model to best_model_RNN_multiclass.h5\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 0.3985 - accuracy: 0.8393 - val_loss: 0.4045 - val_accuracy: 0.8210\n",
      "Epoch 24/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3886 - accuracy: 0.8422\n",
      "Epoch 24: val_loss did not improve from 0.40455\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.3897 - accuracy: 0.8408 - val_loss: 0.4225 - val_accuracy: 0.8148\n",
      "Epoch 25/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3912 - accuracy: 0.8346\n",
      "Epoch 25: val_loss improved from 0.40455 to 0.40114, saving model to best_model_RNN_multiclass.h5\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.3912 - accuracy: 0.8346 - val_loss: 0.4011 - val_accuracy: 0.8395\n",
      "Epoch 26/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3978 - accuracy: 0.8297\n",
      "Epoch 26: val_loss improved from 0.40114 to 0.39445, saving model to best_model_RNN_multiclass.h5\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.3970 - accuracy: 0.8300 - val_loss: 0.3945 - val_accuracy: 0.8333\n",
      "Epoch 27/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3824 - accuracy: 0.8501\n",
      "Epoch 27: val_loss improved from 0.39445 to 0.39027, saving model to best_model_RNN_multiclass.h5\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.3824 - accuracy: 0.8501 - val_loss: 0.3903 - val_accuracy: 0.8580\n",
      "Epoch 28/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3905 - accuracy: 0.8388\n",
      "Epoch 28: val_loss did not improve from 0.39027\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.3877 - accuracy: 0.8439 - val_loss: 0.4305 - val_accuracy: 0.8086\n",
      "Epoch 29/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3802 - accuracy: 0.8470\n",
      "Epoch 29: val_loss improved from 0.39027 to 0.37496, saving model to best_model_RNN_multiclass.h5\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 0.3802 - accuracy: 0.8470 - val_loss: 0.3750 - val_accuracy: 0.8580\n",
      "Epoch 30/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3697 - accuracy: 0.8454\n",
      "Epoch 30: val_loss did not improve from 0.37496\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.3666 - accuracy: 0.8439 - val_loss: 0.4196 - val_accuracy: 0.8457\n",
      "Epoch 31/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3751 - accuracy: 0.8375\n",
      "Epoch 31: val_loss did not improve from 0.37496\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.3723 - accuracy: 0.8393 - val_loss: 0.4401 - val_accuracy: 0.8333\n",
      "Epoch 32/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3942 - accuracy: 0.8391\n",
      "Epoch 32: val_loss did not improve from 0.37496\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.3928 - accuracy: 0.8393 - val_loss: 0.4337 - val_accuracy: 0.8272\n",
      "Epoch 33/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3556 - accuracy: 0.8500\n",
      "Epoch 33: val_loss did not improve from 0.37496\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.3526 - accuracy: 0.8516 - val_loss: 0.4227 - val_accuracy: 0.8272\n",
      "Epoch 34/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3294 - accuracy: 0.8702\n",
      "Epoch 34: val_loss did not improve from 0.37496\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.3294 - accuracy: 0.8702 - val_loss: 0.4019 - val_accuracy: 0.8333\n",
      "Epoch 35/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3321 - accuracy: 0.8640\n",
      "Epoch 35: val_loss did not improve from 0.37496\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.3321 - accuracy: 0.8640 - val_loss: 0.3856 - val_accuracy: 0.8519\n",
      "Epoch 36/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3012 - accuracy: 0.8783\n",
      "Epoch 36: val_loss did not improve from 0.37496\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.3092 - accuracy: 0.8733 - val_loss: 0.3799 - val_accuracy: 0.8642\n",
      "Epoch 37/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3195 - accuracy: 0.8578\n",
      "Epoch 37: val_loss did not improve from 0.37496\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.3195 - accuracy: 0.8578 - val_loss: 0.3877 - val_accuracy: 0.8333\n",
      "Epoch 38/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2876 - accuracy: 0.8734\n",
      "Epoch 38: val_loss improved from 0.37496 to 0.35768, saving model to best_model_RNN_multiclass.h5\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.3012 - accuracy: 0.8671 - val_loss: 0.3577 - val_accuracy: 0.8642\n",
      "Epoch 39/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3180 - accuracy: 0.8641\n",
      "Epoch 39: val_loss did not improve from 0.35768\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.3167 - accuracy: 0.8640 - val_loss: 0.3753 - val_accuracy: 0.8457\n",
      "Epoch 40/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2954 - accuracy: 0.8766\n",
      "Epoch 40: val_loss did not improve from 0.35768\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.2951 - accuracy: 0.8779 - val_loss: 0.4167 - val_accuracy: 0.8086\n",
      "Epoch 41/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2916 - accuracy: 0.8859\n",
      "Epoch 41: val_loss did not improve from 0.35768\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.2896 - accuracy: 0.8872 - val_loss: 0.4372 - val_accuracy: 0.8333\n",
      "Epoch 42/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2913 - accuracy: 0.8797\n",
      "Epoch 42: val_loss did not improve from 0.35768\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.2910 - accuracy: 0.8794 - val_loss: 0.3678 - val_accuracy: 0.8519\n",
      "Epoch 43/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3183 - accuracy: 0.8702\n",
      "Epoch 43: val_loss did not improve from 0.35768\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.3183 - accuracy: 0.8702 - val_loss: 0.3583 - val_accuracy: 0.8395\n",
      "Epoch 44/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3008 - accuracy: 0.8733\n",
      "Epoch 44: val_loss did not improve from 0.35768\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.3008 - accuracy: 0.8733 - val_loss: 0.3583 - val_accuracy: 0.8580\n",
      "Epoch 45/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2839 - accuracy: 0.8764\n",
      "Epoch 45: val_loss did not improve from 0.35768\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.2839 - accuracy: 0.8764 - val_loss: 0.3763 - val_accuracy: 0.8704\n",
      "Epoch 46/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2790 - accuracy: 0.8766\n",
      "Epoch 46: val_loss improved from 0.35768 to 0.35141, saving model to best_model_RNN_multiclass.h5\n",
      "21/21 [==============================] - 1s 35ms/step - loss: 0.2826 - accuracy: 0.8748 - val_loss: 0.3514 - val_accuracy: 0.8519\n",
      "Epoch 47/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2507 - accuracy: 0.8953\n",
      "Epoch 47: val_loss did not improve from 0.35141\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 0.2518 - accuracy: 0.8934 - val_loss: 0.4142 - val_accuracy: 0.8210\n",
      "Epoch 48/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2860 - accuracy: 0.8856\n",
      "Epoch 48: val_loss did not improve from 0.35141\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.2860 - accuracy: 0.8856 - val_loss: 0.3646 - val_accuracy: 0.8457\n",
      "Epoch 49/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2645 - accuracy: 0.8922\n",
      "Epoch 49: val_loss did not improve from 0.35141\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.2632 - accuracy: 0.8918 - val_loss: 0.3934 - val_accuracy: 0.8519\n",
      "Epoch 50/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2535 - accuracy: 0.8980\n",
      "Epoch 50: val_loss did not improve from 0.35141\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 0.2535 - accuracy: 0.8980 - val_loss: 0.4012 - val_accuracy: 0.8457\n",
      "Epoch 51/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.8934\n",
      "Epoch 51: val_loss did not improve from 0.35141\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.2484 - accuracy: 0.8934 - val_loss: 0.3569 - val_accuracy: 0.8395\n",
      "Epoch 52/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2616 - accuracy: 0.8872\n",
      "Epoch 52: val_loss did not improve from 0.35141\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.2616 - accuracy: 0.8872 - val_loss: 0.3888 - val_accuracy: 0.8457\n",
      "Epoch 53/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2273 - accuracy: 0.8995\n",
      "Epoch 53: val_loss improved from 0.35141 to 0.33737, saving model to best_model_RNN_multiclass.h5\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 0.2273 - accuracy: 0.8995 - val_loss: 0.3374 - val_accuracy: 0.8580\n",
      "Epoch 54/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2209 - accuracy: 0.9094\n",
      "Epoch 54: val_loss did not improve from 0.33737\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 0.2215 - accuracy: 0.9088 - val_loss: 0.4167 - val_accuracy: 0.8272\n",
      "Epoch 55/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2111 - accuracy: 0.9178\n",
      "Epoch 55: val_loss did not improve from 0.33737\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.2153 - accuracy: 0.9181 - val_loss: 0.3701 - val_accuracy: 0.8519\n",
      "Epoch 56/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2097 - accuracy: 0.9165\n",
      "Epoch 56: val_loss did not improve from 0.33737\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 0.2097 - accuracy: 0.9165 - val_loss: 0.3478 - val_accuracy: 0.8457\n",
      "Epoch 57/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2065 - accuracy: 0.9109\n",
      "Epoch 57: val_loss did not improve from 0.33737\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.2062 - accuracy: 0.9119 - val_loss: 0.3755 - val_accuracy: 0.8333\n",
      "Epoch 58/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2168 - accuracy: 0.9156\n",
      "Epoch 58: val_loss did not improve from 0.33737\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.2152 - accuracy: 0.9165 - val_loss: 0.3374 - val_accuracy: 0.8704\n",
      "Epoch 59/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2103 - accuracy: 0.9128\n",
      "Epoch 59: val_loss did not improve from 0.33737\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.2087 - accuracy: 0.9134 - val_loss: 0.3823 - val_accuracy: 0.8642\n",
      "Epoch 60/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2035 - accuracy: 0.9250\n",
      "Epoch 60: val_loss did not improve from 0.33737\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.2023 - accuracy: 0.9258 - val_loss: 0.3712 - val_accuracy: 0.8333\n",
      "Epoch 61/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2054 - accuracy: 0.9227\n",
      "Epoch 61: val_loss did not improve from 0.33737\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.2054 - accuracy: 0.9227 - val_loss: 0.4015 - val_accuracy: 0.8148\n",
      "Epoch 62/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1678 - accuracy: 0.9505\n",
      "Epoch 62: val_loss did not improve from 0.33737\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.1678 - accuracy: 0.9505 - val_loss: 0.3800 - val_accuracy: 0.8148\n",
      "Epoch 63/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1672 - accuracy: 0.9505\n",
      "Epoch 63: val_loss did not improve from 0.33737\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.1672 - accuracy: 0.9505 - val_loss: 0.3859 - val_accuracy: 0.8148\n",
      "Epoch 64/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.1953 - accuracy: 0.9125\n",
      "Epoch 64: val_loss did not improve from 0.33737\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.1963 - accuracy: 0.9119 - val_loss: 0.3703 - val_accuracy: 0.8457\n",
      "Epoch 65/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.1951 - accuracy: 0.9260\n",
      "Epoch 65: val_loss did not improve from 0.33737\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 0.1915 - accuracy: 0.9289 - val_loss: 0.4525 - val_accuracy: 0.8086\n",
      "Epoch 66/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1885 - accuracy: 0.9335\n",
      "Epoch 66: val_loss did not improve from 0.33737\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.1885 - accuracy: 0.9335 - val_loss: 0.4064 - val_accuracy: 0.8519\n",
      "Epoch 67/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.1941 - accuracy: 0.9243\n",
      "Epoch 67: val_loss did not improve from 0.33737\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.1916 - accuracy: 0.9274 - val_loss: 0.3965 - val_accuracy: 0.8333\n",
      "Epoch 68/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.1894 - accuracy: 0.9297\n",
      "Epoch 68: val_loss did not improve from 0.33737\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.1946 - accuracy: 0.9289 - val_loss: 0.4390 - val_accuracy: 0.8333\n",
      "Epoch 69/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2026 - accuracy: 0.9266\n",
      "Epoch 69: val_loss did not improve from 0.33737\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.2006 - accuracy: 0.9274 - val_loss: 0.5169 - val_accuracy: 0.7963\n",
      "Epoch 70/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2432 - accuracy: 0.8949\n",
      "Epoch 70: val_loss did not improve from 0.33737\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 0.2432 - accuracy: 0.8949 - val_loss: 0.4025 - val_accuracy: 0.8395\n",
      "Epoch 71/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.1691 - accuracy: 0.9359\n",
      "Epoch 71: val_loss did not improve from 0.33737\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.1653 - accuracy: 0.9366 - val_loss: 0.3772 - val_accuracy: 0.8580\n",
      "Epoch 72/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.1718 - accuracy: 0.9359\n",
      "Epoch 72: val_loss did not improve from 0.33737\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.1680 - accuracy: 0.9382 - val_loss: 0.3928 - val_accuracy: 0.8519\n",
      "Epoch 73/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1952 - accuracy: 0.9243\n",
      "Epoch 73: val_loss did not improve from 0.33737\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.1952 - accuracy: 0.9243 - val_loss: 0.3748 - val_accuracy: 0.8642\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "329c3962-10a9-4105-8ff2-196c457d7cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "best_model = tf.keras.models.load_model('best_model_RNN_multiclass.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40c0c3bf-b5f5-47f4-9dfb-44dfc154ba32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 15ms/step - loss: 0.3981 - accuracy: 0.8571\n",
      "Evaluation Results - Loss: 0.39809393882751465, Accuracy: 0.8571428656578064\n"
     ]
    }
   ],
   "source": [
    "evaluation = best_model.evaluate(X_test_reshaped, y_test)\n",
    "print(f\"Evaluation Results - Loss: {evaluation[0]}, Accuracy: {evaluation[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e6417a9-409f-478e-bf19-3b0795b00598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test_reshaped)\n",
    "y_pred_classes = y_pred.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35bc75c6-131d-4e30-a65d-b954fb741a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(f\"Accuracy Score: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078cfaab-04f9-4fe0-a1b0-be5ece34db41",
   "metadata": {},
   "source": [
    "### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "625a6f16-3102-4a87-b058-f29019e8e8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.89      0.86        65\n",
      "           1       0.84      0.76      0.79        74\n",
      "           2       0.91      0.94      0.92        64\n",
      "\n",
      "    accuracy                           0.86       203\n",
      "   macro avg       0.86      0.86      0.86       203\n",
      "weighted avg       0.86      0.86      0.86       203\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_pred_classes)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f08f54d7-885d-4853-aea8-5ef6b9ff2a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2767b4db-1430-4ecc-b803-d237dad90498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“Python(gpu)”",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
