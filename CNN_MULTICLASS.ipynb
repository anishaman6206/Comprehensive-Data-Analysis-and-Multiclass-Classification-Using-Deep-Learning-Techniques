{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e476d745-cade-4f2f-86e7-3325c60ad4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68537fe-5e1f-49b3-9714-a02f4240a6b6",
   "metadata": {},
   "source": [
    "### Importing saved data, analyzed on LSTM model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4db1d6ee-94c6-4c36-96f5-dadbf5dcd799",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('clean_df_MULTICLASS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7d77991-ccbe-4b7b-8d04-562ef1c2a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b0e1258-5d6c-4517-ad54-a1e23e17fb77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CAT1</th>\n",
       "      <th>ORD1</th>\n",
       "      <th>ORD2</th>\n",
       "      <th>ORD3</th>\n",
       "      <th>ORD4</th>\n",
       "      <th>ORD5</th>\n",
       "      <th>ORD6</th>\n",
       "      <th>ORD7</th>\n",
       "      <th>ORD8</th>\n",
       "      <th>ORD9</th>\n",
       "      <th>ORD10</th>\n",
       "      <th>ORD11</th>\n",
       "      <th>ORD12</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.090</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.967</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.967</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.062</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.062</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>0</td>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.479</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>0</td>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.288</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>0</td>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.168</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>0</td>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.389</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>0</td>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.505</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1012 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CAT1     ORD1  ORD2   ORD3   ORD4   ORD5  ORD6   ORD7  ORD8  ORD9  \\\n",
       "0        0  0.00632  18.0   2.31  0.538  6.575  65.2  4.090     1   296   \n",
       "1        0  0.02731   0.0   7.07  0.469  6.421  78.9  4.967     2   242   \n",
       "2        0  0.02729   0.0   7.07  0.469  7.185  61.1  4.967     2   242   \n",
       "3        0  0.03237   0.0   2.18  0.458  6.998  45.8  6.062     3   222   \n",
       "4        0  0.06905   0.0   2.18  0.458  7.147  54.2  6.062     3   222   \n",
       "...    ...      ...   ...    ...    ...    ...   ...    ...   ...   ...   \n",
       "1007     0  0.06263   0.0  11.93  0.573  6.593  69.1  2.479     1   273   \n",
       "1008     0  0.04527   0.0  11.93  0.573  6.120  76.7  2.288     1   273   \n",
       "1009     0  0.06076   0.0  11.93  0.573  6.976  91.0  2.168     1   273   \n",
       "1010     0  0.10959   0.0  11.93  0.573  6.794  89.3  2.389     1   273   \n",
       "1011     0  0.04741   0.0  11.93  0.573  6.030  80.8  2.505     1   273   \n",
       "\n",
       "      ORD10   ORD11  ORD12  PRICE  \n",
       "0      15.3  396.90   4.98      2  \n",
       "1      17.8  396.90   9.14      1  \n",
       "2      17.8  392.83   4.03      2  \n",
       "3      18.7  394.63   2.94      2  \n",
       "4      18.7  396.90   5.33      2  \n",
       "...     ...     ...    ...    ...  \n",
       "1007   21.0  391.99   9.67      1  \n",
       "1008   21.0  396.90   9.08      1  \n",
       "1009   21.0  396.90   5.64      2  \n",
       "1010   21.0  393.45   6.48      1  \n",
       "1011   21.0  396.90   7.88      0  \n",
       "\n",
       "[1012 rows x 14 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4355bf42-4ce5-4a80-a9a8-cde2302592e1",
   "metadata": {},
   "source": [
    "### Train,test,split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "317f7181-168e-433b-8af7-9b4a4b400943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,0:13],df.iloc[:,-1],test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03ba85e1-cc4f-4a2d-87f9-7061c2ffb121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((809, 13), (203, 13), (809,), (203,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1282ef-07a0-4e52-bbae-e8fc2e688820",
   "metadata": {},
   "source": [
    "### Making k-1 dummy variable of 'ORD8' categorical column using OneHotEncoder since nominal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e380402-6973-46c2-ab12-ec027b95a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "transformer = ColumnTransformer(transformers=[\n",
    "    ('tnf1' ,OneHotEncoder(sparse_output=False,drop='first'),['ORD8'])\n",
    "     ],remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef955238-3063-434b-861b-8db33aed5d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = transformer.fit_transform(X_train)\n",
    "X_test_new = transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cff8d1e-36cc-4762-9ff5-c01979357e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((809, 20), (203, 20), (809,), (203,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new.shape, X_test_new.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7787117-7138-4970-b71d-1b353953d016",
   "metadata": {},
   "source": [
    "### Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ffb791e-bf61-453f-adfc-2ee48e631ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_new)\n",
    "X_test_scaled = scaler.transform(X_test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6012d4-ee21-447d-a5c5-428d017c3256",
   "metadata": {},
   "source": [
    "### Reshape data for CNN [samples, features, channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec789055-b515-46da-990a-3035e9cf2fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f945fd-be2f-4a20-a23c-7df3981b66ec",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39354665-508d-406f-b352-b4ea3e7ce474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9967ea71-492a-489e-9c7a-e2764bf093bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 18, 64)            256       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 9, 64)            0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 7, 128)            24704     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 3, 128)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 384)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                19250     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 153       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,363\n",
      "Trainable params: 44,363\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3352cd74-1b31-4fc4-8d33-13e33933a8f6",
   "metadata": {},
   "source": [
    "### callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c6bdee9-31f9-42c5-bf1d-bc30b55a446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "checkpoint = ModelCheckpoint('best_model_CNN_multiclass.h5', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4958d0-169f-4d83-b426-5431426ddcd9",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8b7a85a-4d77-4555-bfcd-5b4b09446410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9756 - accuracy: 0.5147\n",
      "Epoch 1: val_loss improved from inf to 0.89123, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 22s 53ms/step - loss: 0.9756 - accuracy: 0.5147 - val_loss: 0.8912 - val_accuracy: 0.5864\n",
      "Epoch 2/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.8436 - accuracy: 0.5938\n",
      "Epoch 2: val_loss improved from 0.89123 to 0.79368, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.8221 - accuracy: 0.6151 - val_loss: 0.7937 - val_accuracy: 0.6358\n",
      "Epoch 3/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.7574 - accuracy: 0.6719\n",
      "Epoch 3: val_loss improved from 0.79368 to 0.74749, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.7575 - accuracy: 0.6723 - val_loss: 0.7475 - val_accuracy: 0.6481\n",
      "Epoch 4/100\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.7206 - accuracy: 0.7000\n",
      "Epoch 4: val_loss improved from 0.74749 to 0.69725, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.7189 - accuracy: 0.6986 - val_loss: 0.6972 - val_accuracy: 0.6728\n",
      "Epoch 5/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.6478 - accuracy: 0.7684\n",
      "Epoch 5: val_loss improved from 0.69725 to 0.62565, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6610 - accuracy: 0.7527 - val_loss: 0.6257 - val_accuracy: 0.7099\n",
      "Epoch 6/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.6376 - accuracy: 0.7303\n",
      "Epoch 6: val_loss did not improve from 0.62565\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6366 - accuracy: 0.7295 - val_loss: 0.6409 - val_accuracy: 0.7099\n",
      "Epoch 7/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.5836 - accuracy: 0.7537\n",
      "Epoch 7: val_loss improved from 0.62565 to 0.57890, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5951 - accuracy: 0.7450 - val_loss: 0.5789 - val_accuracy: 0.7346\n",
      "Epoch 8/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.5570 - accuracy: 0.7715\n",
      "Epoch 8: val_loss improved from 0.57890 to 0.54915, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.5687 - accuracy: 0.7666 - val_loss: 0.5491 - val_accuracy: 0.7531\n",
      "Epoch 9/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.5684 - accuracy: 0.7886\n",
      "Epoch 9: val_loss improved from 0.54915 to 0.52661, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5706 - accuracy: 0.7805 - val_loss: 0.5266 - val_accuracy: 0.7840\n",
      "Epoch 10/100\n",
      "13/21 [=================>............] - ETA: 0s - loss: 0.5438 - accuracy: 0.7837\n",
      "Epoch 10: val_loss improved from 0.52661 to 0.48439, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5410 - accuracy: 0.7852 - val_loss: 0.4844 - val_accuracy: 0.7901\n",
      "Epoch 11/100\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 0.5163 - accuracy: 0.7768\n",
      "Epoch 11: val_loss did not improve from 0.48439\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5313 - accuracy: 0.7697 - val_loss: 0.4907 - val_accuracy: 0.7593\n",
      "Epoch 12/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.4950 - accuracy: 0.7978\n",
      "Epoch 12: val_loss did not improve from 0.48439\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.5192 - accuracy: 0.7883 - val_loss: 0.4878 - val_accuracy: 0.7901\n",
      "Epoch 13/100\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.5065 - accuracy: 0.7875\n",
      "Epoch 13: val_loss improved from 0.48439 to 0.47059, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.5074 - accuracy: 0.7960 - val_loss: 0.4706 - val_accuracy: 0.7778\n",
      "Epoch 14/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.4941 - accuracy: 0.7886\n",
      "Epoch 14: val_loss improved from 0.47059 to 0.46045, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4988 - accuracy: 0.7867 - val_loss: 0.4604 - val_accuracy: 0.8148\n",
      "Epoch 15/100\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.4800 - accuracy: 0.8188\n",
      "Epoch 15: val_loss improved from 0.46045 to 0.44799, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4724 - accuracy: 0.8176 - val_loss: 0.4480 - val_accuracy: 0.8025\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4931 - accuracy: 0.7991\n",
      "Epoch 16: val_loss did not improve from 0.44799\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4931 - accuracy: 0.7991 - val_loss: 0.4659 - val_accuracy: 0.7963\n",
      "Epoch 17/100\n",
      "13/21 [=================>............] - ETA: 0s - loss: 0.4573 - accuracy: 0.8173\n",
      "Epoch 17: val_loss did not improve from 0.44799\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4848 - accuracy: 0.7975 - val_loss: 0.4909 - val_accuracy: 0.7593\n",
      "Epoch 18/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.4828 - accuracy: 0.7988\n",
      "Epoch 18: val_loss did not improve from 0.44799\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.4628 - accuracy: 0.8068 - val_loss: 0.4576 - val_accuracy: 0.8025\n",
      "Epoch 19/100\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.4497 - accuracy: 0.8042\n",
      "Epoch 19: val_loss improved from 0.44799 to 0.42853, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.4602 - accuracy: 0.8099 - val_loss: 0.4285 - val_accuracy: 0.8025\n",
      "Epoch 20/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4541 - accuracy: 0.8207\n",
      "Epoch 20: val_loss did not improve from 0.42853\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4541 - accuracy: 0.8207 - val_loss: 0.4342 - val_accuracy: 0.7778\n",
      "Epoch 21/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4388 - accuracy: 0.8223\n",
      "Epoch 21: val_loss improved from 0.42853 to 0.42560, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 0s 16ms/step - loss: 0.4388 - accuracy: 0.8223 - val_loss: 0.4256 - val_accuracy: 0.8272\n",
      "Epoch 22/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4402 - accuracy: 0.8141\n",
      "Epoch 22: val_loss did not improve from 0.42560\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4434 - accuracy: 0.8130 - val_loss: 0.4320 - val_accuracy: 0.8025\n",
      "Epoch 23/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.4187 - accuracy: 0.8143\n",
      "Epoch 23: val_loss improved from 0.42560 to 0.42144, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4346 - accuracy: 0.8114 - val_loss: 0.4214 - val_accuracy: 0.7963\n",
      "Epoch 24/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4304 - accuracy: 0.8322\n",
      "Epoch 24: val_loss improved from 0.42144 to 0.41937, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.4225 - accuracy: 0.8346 - val_loss: 0.4194 - val_accuracy: 0.8333\n",
      "Epoch 25/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4292 - accuracy: 0.8289\n",
      "Epoch 25: val_loss improved from 0.41937 to 0.38011, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 0s 16ms/step - loss: 0.4233 - accuracy: 0.8346 - val_loss: 0.3801 - val_accuracy: 0.8704\n",
      "Epoch 26/100\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 0.4118 - accuracy: 0.8348\n",
      "Epoch 26: val_loss did not improve from 0.38011\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4170 - accuracy: 0.8362 - val_loss: 0.4012 - val_accuracy: 0.8272\n",
      "Epoch 27/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.4067 - accuracy: 0.8457\n",
      "Epoch 27: val_loss did not improve from 0.38011\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4035 - accuracy: 0.8423 - val_loss: 0.4068 - val_accuracy: 0.8086\n",
      "Epoch 28/100\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.4104 - accuracy: 0.8354\n",
      "Epoch 28: val_loss did not improve from 0.38011\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.4004 - accuracy: 0.8423 - val_loss: 0.3946 - val_accuracy: 0.8272\n",
      "Epoch 29/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4016 - accuracy: 0.8439\n",
      "Epoch 29: val_loss did not improve from 0.38011\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4016 - accuracy: 0.8439 - val_loss: 0.3872 - val_accuracy: 0.8210\n",
      "Epoch 30/100\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.4080 - accuracy: 0.8333\n",
      "Epoch 30: val_loss did not improve from 0.38011\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4068 - accuracy: 0.8284 - val_loss: 0.4125 - val_accuracy: 0.8333\n",
      "Epoch 31/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4059 - accuracy: 0.8240\n",
      "Epoch 31: val_loss did not improve from 0.38011\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3968 - accuracy: 0.8300 - val_loss: 0.3877 - val_accuracy: 0.8580\n",
      "Epoch 32/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.4031 - accuracy: 0.8327\n",
      "Epoch 32: val_loss improved from 0.38011 to 0.37553, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 0s 17ms/step - loss: 0.4115 - accuracy: 0.8238 - val_loss: 0.3755 - val_accuracy: 0.8519\n",
      "Epoch 33/100\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 0.4054 - accuracy: 0.8170\n",
      "Epoch 33: val_loss did not improve from 0.37553\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3948 - accuracy: 0.8253 - val_loss: 0.3816 - val_accuracy: 0.8519\n",
      "Epoch 34/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3905 - accuracy: 0.8391\n",
      "Epoch 34: val_loss improved from 0.37553 to 0.36874, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 0s 17ms/step - loss: 0.3894 - accuracy: 0.8393 - val_loss: 0.3687 - val_accuracy: 0.8519\n",
      "Epoch 35/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.3681 - accuracy: 0.8555\n",
      "Epoch 35: val_loss did not improve from 0.36874\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3627 - accuracy: 0.8563 - val_loss: 0.3838 - val_accuracy: 0.8272\n",
      "Epoch 36/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.3565 - accuracy: 0.8418\n",
      "Epoch 36: val_loss improved from 0.36874 to 0.36724, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.3634 - accuracy: 0.8377 - val_loss: 0.3672 - val_accuracy: 0.8519\n",
      "Epoch 37/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.3636 - accuracy: 0.8652\n",
      "Epoch 37: val_loss did not improve from 0.36724\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3725 - accuracy: 0.8594 - val_loss: 0.3723 - val_accuracy: 0.8765\n",
      "Epoch 38/100\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3636 - accuracy: 0.8500\n",
      "Epoch 38: val_loss did not improve from 0.36724\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3713 - accuracy: 0.8485 - val_loss: 0.3717 - val_accuracy: 0.8457\n",
      "Epoch 39/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3448 - accuracy: 0.8779\n",
      "Epoch 39: val_loss did not improve from 0.36724\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3448 - accuracy: 0.8779 - val_loss: 0.3848 - val_accuracy: 0.8148\n",
      "Epoch 40/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.3794 - accuracy: 0.8398\n",
      "Epoch 40: val_loss did not improve from 0.36724\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3807 - accuracy: 0.8377 - val_loss: 0.4074 - val_accuracy: 0.8210\n",
      "Epoch 41/100\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3442 - accuracy: 0.8542\n",
      "Epoch 41: val_loss did not improve from 0.36724\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3478 - accuracy: 0.8547 - val_loss: 0.3994 - val_accuracy: 0.8210\n",
      "Epoch 42/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3525 - accuracy: 0.8470\n",
      "Epoch 42: val_loss did not improve from 0.36724\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3525 - accuracy: 0.8470 - val_loss: 0.4452 - val_accuracy: 0.8272\n",
      "Epoch 43/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.3167 - accuracy: 0.8809\n",
      "Epoch 43: val_loss improved from 0.36724 to 0.35341, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3126 - accuracy: 0.8794 - val_loss: 0.3534 - val_accuracy: 0.8519\n",
      "Epoch 44/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2967 - accuracy: 0.8770\n",
      "Epoch 44: val_loss did not improve from 0.35341\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3225 - accuracy: 0.8702 - val_loss: 0.3651 - val_accuracy: 0.8395\n",
      "Epoch 45/100\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3069 - accuracy: 0.8646\n",
      "Epoch 45: val_loss did not improve from 0.35341\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3157 - accuracy: 0.8640 - val_loss: 0.3739 - val_accuracy: 0.8519\n",
      "Epoch 46/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.3518 - accuracy: 0.8585\n",
      "Epoch 46: val_loss did not improve from 0.35341\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3426 - accuracy: 0.8655 - val_loss: 0.3894 - val_accuracy: 0.8210\n",
      "Epoch 47/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3327 - accuracy: 0.8578\n",
      "Epoch 47: val_loss did not improve from 0.35341\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3327 - accuracy: 0.8578 - val_loss: 0.3739 - val_accuracy: 0.8642\n",
      "Epoch 48/100\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3157 - accuracy: 0.8667\n",
      "Epoch 48: val_loss improved from 0.35341 to 0.34285, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.3201 - accuracy: 0.8609 - val_loss: 0.3429 - val_accuracy: 0.8519\n",
      "Epoch 49/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.3322 - accuracy: 0.8652\n",
      "Epoch 49: val_loss did not improve from 0.34285\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3342 - accuracy: 0.8733 - val_loss: 0.3835 - val_accuracy: 0.8395\n",
      "Epoch 50/100\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3019 - accuracy: 0.8625\n",
      "Epoch 50: val_loss did not improve from 0.34285\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3088 - accuracy: 0.8578 - val_loss: 0.4073 - val_accuracy: 0.8210\n",
      "Epoch 51/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3100 - accuracy: 0.8799\n",
      "Epoch 51: val_loss did not improve from 0.34285\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3029 - accuracy: 0.8825 - val_loss: 0.3799 - val_accuracy: 0.8395\n",
      "Epoch 52/100\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3447 - accuracy: 0.8583\n",
      "Epoch 52: val_loss did not improve from 0.34285\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3167 - accuracy: 0.8748 - val_loss: 0.3997 - val_accuracy: 0.8148\n",
      "Epoch 53/100\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2972 - accuracy: 0.8875\n",
      "Epoch 53: val_loss did not improve from 0.34285\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2881 - accuracy: 0.8903 - val_loss: 0.3714 - val_accuracy: 0.8333\n",
      "Epoch 54/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2873 - accuracy: 0.8770\n",
      "Epoch 54: val_loss did not improve from 0.34285\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2852 - accuracy: 0.8825 - val_loss: 0.3623 - val_accuracy: 0.8333\n",
      "Epoch 55/100\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2608 - accuracy: 0.9042\n",
      "Epoch 55: val_loss did not improve from 0.34285\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2787 - accuracy: 0.8980 - val_loss: 0.3542 - val_accuracy: 0.8395\n",
      "Epoch 56/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2706 - accuracy: 0.8734\n",
      "Epoch 56: val_loss did not improve from 0.34285\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2694 - accuracy: 0.8764 - val_loss: 0.4089 - val_accuracy: 0.8333\n",
      "Epoch 57/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2817 - accuracy: 0.8945\n",
      "Epoch 57: val_loss did not improve from 0.34285\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2762 - accuracy: 0.8934 - val_loss: 0.3831 - val_accuracy: 0.8519\n",
      "Epoch 58/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.2787 - accuracy: 0.8924\n",
      "Epoch 58: val_loss did not improve from 0.34285\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2834 - accuracy: 0.8918 - val_loss: 0.3852 - val_accuracy: 0.8580\n",
      "Epoch 59/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2593 - accuracy: 0.8984\n",
      "Epoch 59: val_loss improved from 0.34285 to 0.34032, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.2720 - accuracy: 0.8903 - val_loss: 0.3403 - val_accuracy: 0.8704\n",
      "Epoch 60/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2639 - accuracy: 0.9057\n",
      "Epoch 60: val_loss did not improve from 0.34032\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2639 - accuracy: 0.9057 - val_loss: 0.3625 - val_accuracy: 0.8519\n",
      "Epoch 61/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2598 - accuracy: 0.8906\n",
      "Epoch 61: val_loss improved from 0.34032 to 0.32272, saving model to best_model_CNN_multiclass.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.2607 - accuracy: 0.8887 - val_loss: 0.3227 - val_accuracy: 0.9012\n",
      "Epoch 62/100\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2550 - accuracy: 0.8979\n",
      "Epoch 62: val_loss did not improve from 0.32272\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2694 - accuracy: 0.8918 - val_loss: 0.3511 - val_accuracy: 0.8580\n",
      "Epoch 63/100\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2622 - accuracy: 0.9146\n",
      "Epoch 63: val_loss did not improve from 0.32272\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2786 - accuracy: 0.8995 - val_loss: 0.4093 - val_accuracy: 0.8704\n",
      "Epoch 64/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2592 - accuracy: 0.9004\n",
      "Epoch 64: val_loss did not improve from 0.32272\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2563 - accuracy: 0.9011 - val_loss: 0.3696 - val_accuracy: 0.8457\n",
      "Epoch 65/100\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2200 - accuracy: 0.9021\n",
      "Epoch 65: val_loss did not improve from 0.32272\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2223 - accuracy: 0.8980 - val_loss: 0.4319 - val_accuracy: 0.8272\n",
      "Epoch 66/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.2332 - accuracy: 0.9044\n",
      "Epoch 66: val_loss did not improve from 0.32272\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.2386 - accuracy: 0.9088 - val_loss: 0.3600 - val_accuracy: 0.8765\n",
      "Epoch 67/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2577 - accuracy: 0.8926\n",
      "Epoch 67: val_loss did not improve from 0.32272\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2659 - accuracy: 0.8903 - val_loss: 0.3677 - val_accuracy: 0.8580\n",
      "Epoch 68/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2470 - accuracy: 0.8906\n",
      "Epoch 68: val_loss did not improve from 0.32272\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2582 - accuracy: 0.8934 - val_loss: 0.3635 - val_accuracy: 0.8765\n",
      "Epoch 69/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2866 - accuracy: 0.8867\n",
      "Epoch 69: val_loss did not improve from 0.32272\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2742 - accuracy: 0.8872 - val_loss: 0.3496 - val_accuracy: 0.8704\n",
      "Epoch 70/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.2529 - accuracy: 0.8934\n",
      "Epoch 70: val_loss did not improve from 0.32272\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2442 - accuracy: 0.8995 - val_loss: 0.3570 - val_accuracy: 0.8519\n",
      "Epoch 71/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2289 - accuracy: 0.9079\n",
      "Epoch 71: val_loss did not improve from 0.32272\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2317 - accuracy: 0.9073 - val_loss: 0.3727 - val_accuracy: 0.8519\n",
      "Epoch 72/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2389 - accuracy: 0.8887\n",
      "Epoch 72: val_loss did not improve from 0.32272\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2385 - accuracy: 0.8949 - val_loss: 0.4091 - val_accuracy: 0.8580\n",
      "Epoch 73/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2538 - accuracy: 0.8906\n",
      "Epoch 73: val_loss did not improve from 0.32272\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2525 - accuracy: 0.8918 - val_loss: 0.4417 - val_accuracy: 0.8519\n",
      "Epoch 74/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2268 - accuracy: 0.9109\n",
      "Epoch 74: val_loss did not improve from 0.32272\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.2293 - accuracy: 0.9073 - val_loss: 0.3300 - val_accuracy: 0.8704\n",
      "Epoch 75/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.2392 - accuracy: 0.8879\n",
      "Epoch 75: val_loss did not improve from 0.32272\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2463 - accuracy: 0.8934 - val_loss: 0.3806 - val_accuracy: 0.8889\n",
      "Epoch 76/100\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2415 - accuracy: 0.9000\n",
      "Epoch 76: val_loss did not improve from 0.32272\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2347 - accuracy: 0.9011 - val_loss: 0.3571 - val_accuracy: 0.8519\n",
      "Epoch 77/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2289 - accuracy: 0.9062\n",
      "Epoch 77: val_loss did not improve from 0.32272\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2484 - accuracy: 0.8918 - val_loss: 0.3878 - val_accuracy: 0.8642\n",
      "Epoch 78/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2373 - accuracy: 0.9016\n",
      "Epoch 78: val_loss did not improve from 0.32272\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2372 - accuracy: 0.9011 - val_loss: 0.3426 - val_accuracy: 0.8765\n",
      "Epoch 79/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.2241 - accuracy: 0.9080\n",
      "Epoch 79: val_loss did not improve from 0.32272\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2197 - accuracy: 0.9119 - val_loss: 0.3320 - val_accuracy: 0.8642\n",
      "Epoch 80/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.1919 - accuracy: 0.9238\n",
      "Epoch 80: val_loss did not improve from 0.32272\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.1993 - accuracy: 0.9165 - val_loss: 0.3806 - val_accuracy: 0.8580\n",
      "Epoch 81/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2184 - accuracy: 0.8984\n",
      "Epoch 81: val_loss did not improve from 0.32272\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2117 - accuracy: 0.9026 - val_loss: 0.3747 - val_accuracy: 0.8519\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9001affa-6026-4381-8e9f-757aa45ac6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "best_model = tf.keras.models.load_model('best_model_CNN_multiclass.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db1fe53b-940a-4db5-a7a1-86e7cf98273f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 45ms/step - loss: 0.5041 - accuracy: 0.8719\n",
      "Evaluation Results - Loss: 0.5041300654411316, Accuracy: 0.871921181678772\n"
     ]
    }
   ],
   "source": [
    "evaluation = best_model.evaluate(X_test_reshaped, y_test)\n",
    "print(f\"Evaluation Results - Loss: {evaluation[0]}, Accuracy: {evaluation[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9632250-8176-4e01-a4b8-cb53b41bd10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test_reshaped)\n",
    "y_pred_classes = y_pred.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d87cb5f7-5358-497a-9282-ec8ceb65212f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 87.192118226601\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(f\"Accuracy Score: {accuracy*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79edad49-6e2b-42c0-bca1-682a41fe180b",
   "metadata": {},
   "source": [
    "### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c135bc8-5e06-4b65-968f-0689885b3d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86        65\n",
      "           1       0.82      0.91      0.86        74\n",
      "           2       0.92      0.88      0.90        64\n",
      "\n",
      "    accuracy                           0.87       203\n",
      "   macro avg       0.88      0.87      0.87       203\n",
      "weighted avg       0.88      0.87      0.87       203\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_pred_classes)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89a14e0e-c0b6-4ecd-bebc-dbe027b134db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwgAAAJwCAYAAAAtA0YPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABDqElEQVR4nO3de3yP9f/H8edn4rPZ0bCN2MyZCCEtOdWQSjSK6lujc79RDNU6OVRWyiE5pYSUcxGdJMukEJMcKjnVKjanbIx9aPv8/jCX6xNqH22fQ5/Hvdt1u9n7uj7X9brWbeW15/t9XRa73W4XAAAAAEjyc3cBAAAAADwHDQIAAAAAAw0CAAAAAAMNAgAAAAADDQIAAAAAAw0CAAAAAAMNAgAAAAADDQIAAAAAAw0CAAAAAAMNAgCcx44dO9SpUyeFhobKYrFo8eLFJXr+n3/+WRaLRTNmzCjR83qz9u3bq3379u4uAwB8Hg0CAI+1a9cuPfjgg6pZs6b8/f0VEhKi1q1b69VXX9WJEydK9dqJiYnasmWLXnjhBc2aNUstWrQo1eu5Up8+fWSxWBQSEnLe7+OOHTtksVhksVj0yiuvOH3+vXv3atiwYdq0aVMJVAsAcLVL3F0AAJzPRx99pFtvvVVWq1V33323GjVqpJMnT2r16tUaMmSItm3bpqlTp5bKtU+cOKE1a9boqaeeUr9+/UrlGjExMTpx4oTKli1bKuf/J5dccomOHz+upUuX6rbbbnPY9+6778rf31/5+fkXde69e/dq+PDhqlGjhpo2bVrsz3322WcXdT0AQMmiQQDgcfbs2aPevXsrJiZGaWlpqlKlirEvKSlJO3fu1EcffVRq1z9w4IAkKSwsrNSuYbFY5O/vX2rn/ydWq1WtW7fWnDlzzmkQZs+erRtvvFHvvfeeS2o5fvy4ypcvr3LlyrnkegCAv8cUIwAeZ9SoUTp27JimTZvm0BycUbt2bT366KPG13/++aeee+451apVS1arVTVq1NCTTz4pm83m8LkaNWropptu0urVq3XllVfK399fNWvW1Ntvv20cM2zYMMXExEiShgwZIovFoho1akg6PTXnzJ/Nhg0bJovF4jC2fPlyXXPNNQoLC1NQUJDq1aunJ5980th/oTUIaWlpatOmjQIDAxUWFqZu3brphx9+OO/1du7cqT59+igsLEyhoaHq27evjh8/fuFv7F/ccccd+uSTT3TkyBFjbP369dqxY4fuuOOOc44/fPiwBg8erMaNGysoKEghISHq0qWLvvvuO+OYlStXqmXLlpKkvn37GlOVztxn+/bt1ahRI2VkZKht27YqX7688X356xqExMRE+fv7n3P/nTt3VoUKFbR3795i3ysAoPhoEAB4nKVLl6pmzZq6+uqri3X8fffdp2effVZXXHGFxo4dq3bt2ik1NVW9e/c+59idO3eqZ8+e6tixo0aPHq0KFSqoT58+2rZtmyQpISFBY8eOlSTdfvvtmjVrlsaNG+dU/du2bdNNN90km82mESNGaPTo0br55pv11Vdf/e3nPv/8c3Xu3Fn79+/XsGHDlJycrK+//lqtW7fWzz//fM7xt912m44eParU1FTddtttmjFjhoYPH17sOhMSEmSxWPT+++8bY7Nnz1b9+vV1xRVXnHP87t27tXjxYt10000aM2aMhgwZoi1btqhdu3bGX9YbNGigESNGSJIeeOABzZo1S7NmzVLbtm2N8xw6dEhdunRR06ZNNW7cOHXo0OG89b366quqXLmyEhMTVVBQIEl6/fXX9dlnn+m1115T1apVi32vAAAn2AHAg+Tk5Ngl2bt161as4zdt2mSXZL/vvvscxgcPHmyXZE9LSzPGYmJi7JLsq1atMsb2799vt1qt9kGDBhlje/bssUuyv/zyyw7nTExMtMfExJxTw9ChQ+3m/5yOHTvWLsl+4MCBC9Z95hrTp083xpo2bWqPiIiwHzp0yBj77rvv7H5+fva77777nOvdc889Due85ZZb7BUrVrzgNc33ERgYaLfb7faePXvar7vuOrvdbrcXFBTYo6Ki7MOHDz/v9yA/P99eUFBwzn1YrVb7iBEjjLH169efc29ntGvXzi7JPmXKlPPua9euncPYsmXL7JLszz//vH337t32oKAge/fu3f/xHgEAF48EAYBHyc3NlSQFBwcX6/iPP/5YkpScnOwwPmjQIEk6Z61Cw4YN1aZNG+PrypUrq169etq9e/dF1/xXZ9YufPDBByosLCzWZ/bt26dNmzapT58+Cg8PN8Yvv/xydezY0bhPs4ceesjh6zZt2ujQoUPG97A47rjjDq1cuVJZWVlKS0tTVlbWeacXSafXLfj5nf7fRkFBgQ4dOmRMn9q4cWOxr2m1WtW3b99iHdupUyc9+OCDGjFihBISEuTv76/XX3+92NcCADiPBgGARwkJCZEkHT16tFjH//LLL/Lz81Pt2rUdxqOiohQWFqZffvnFYTw6Ovqcc1SoUEF//PHHRVZ8rl69eql169a67777FBkZqd69e2v+/Pl/2yycqbNevXrn7GvQoIEOHjyovLw8h/G/3kuFChUkyal7ueGGGxQcHKx58+bp3XffVcuWLc/5Xp5RWFiosWPHqk6dOrJarapUqZIqV66szZs3Kycnp9jXvPTSS51akPzKK68oPDxcmzZt0vjx4xUREVHszwIAnEeDAMCjhISEqGrVqtq6datTn/vrIuELKVOmzHnH7Xb7RV/jzPz4MwICArRq1Sp9/vnnuuuuu7R582b16tVLHTt2POfYf+Pf3MsZVqtVCQkJmjlzphYtWnTB9ECSRo4cqeTkZLVt21bvvPOOli1bpuXLl+uyyy4rdlIinf7+OOPbb7/V/v37JUlbtmxx6rMAAOfRIADwODfddJN27dqlNWvW/OOxMTExKiws1I4dOxzGs7OzdeTIEeOJRCWhQoUKDk/8OeOvKYUk+fn56brrrtOYMWP0/fff64UXXlBaWpq++OKL8577TJ3bt28/Z9+PP/6oSpUqKTAw8N/dwAXccccd+vbbb3X06NHzLuw+Y+HCherQoYOmTZum3r17q1OnToqPjz/ne1LcZq048vLy1LdvXzVs2FAPPPCARo0apfXr15fY+QEA56JBAOBxHnvsMQUGBuq+++5Tdnb2Oft37dqlV199VdLpKTKSznnS0JgxYyRJN954Y4nVVatWLeXk5Gjz5s3G2L59+7Ro0SKH4w4fPnzOZ8+8MOyvj149o0qVKmratKlmzpzp8BfurVu36rPPPjPuszR06NBBzz33nCZMmKCoqKgLHlemTJlz0okFCxbo999/dxg708icr5ly1uOPP67MzEzNnDlTY8aMUY0aNZSYmHjB7yMA4N/jRWkAPE6tWrU0e/Zs9erVSw0aNHB4k/LXX3+tBQsWqE+fPpKkJk2aKDExUVOnTtWRI0fUrl07ffPNN5o5c6a6d+9+wUdoXozevXvr8ccf1y233KJHHnlEx48f1+TJk1W3bl2HRbojRozQqlWrdOONNyomJkb79+/XpEmTVK1aNV1zzTUXPP/LL7+sLl26KC4uTvfee69OnDih1157TaGhoRo2bFiJ3cdf+fn56emnn/7H42666SaNGDFCffv21dVXX60tW7bo3XffVc2aNR2Oq1WrlsLCwjRlyhQFBwcrMDBQrVq1UmxsrFN1paWladKkSRo6dKjx2NXp06erffv2euaZZzRq1CinzgcAKB4SBAAe6eabb9bmzZvVs2dPffDBB0pKStITTzyhn3/+WaNHj9b48eONY998800NHz5c69ev14ABA5SWlqaUlBTNnTu3RGuqWLGiFi1apPLly+uxxx7TzJkzlZqaqq5du55Te3R0tN566y0lJSVp4sSJatu2rdLS0hQaGnrB88fHx+vTTz9VxYoV9eyzz+qVV17RVVddpa+++srpv1yXhieffFKDBg3SsmXL9Oijj2rjxo366KOPVL16dYfjypYtq5kzZ6pMmTJ66KGHdPvttys9Pd2pax09elT33HOPmjVrpqeeesoYb9OmjR599FGNHj1aa9euLZH7AgA4stidWc0GAAAA4D+NBAEAAACAgQYBAAAAgIEGAQAAAICBBgEAAACAgQYBAAAAgIEGAQAAAICBBgEAAACA4T/5JuWo+xe6uwTAK61/qes/HwTgHJVDrO4uAfA6/h78t9CAZv1cdq0T305w2bWKiwQBAAAAgMGDezcAAADADSy+/Tt03757AAAAAA5IEAAAAAAzi8XdFbgVCQIAAAAAAwkCAAAAYMYaBAAAAAA4jQQBAAAAMGMNAgAAAACcRoIAAAAAmLEGAQAAAABOI0EAAAAAzFiDAAAAAACnkSAAAAAAZqxBAAAAAIDTaBAAAAAAGJhiBAAAAJixSBkAAAAATiNBAAAAAMxYpAwAAAAAp5EgAAAAAGasQQAAAACA00gQAAAAADPWIAAAAADAaSQIAAAAgBlrEAAAAADgNBIEAAAAwIw1CAAAAABwGgkCAAAAYEaCAAAAAACnkSAAAAAAZn48xQgAAAAAJJEgAAAAAI5YgwAAAAAAp9EgAAAAADAwxQgAAAAws7BIGQAAAAAkkSAAAAAAjlikDAAAAACnkSAAAAAAZqxBAAAAAIDTSBAAAAAAM9YgAAAAAMBpJAgAAACAGWsQAAAAAOA0EgQAAADAjDUIAAAAAHAaCQIAAABgxhoEAAAAADiNBAEAAAAwYw0CAAAAAJxGggAAAACYsQYBAAAAAE6jQQAAAADMLH6u25z0+++/63//+58qVqyogIAANW7cWBs2bDD22+12Pfvss6pSpYoCAgIUHx+vHTt2OHUNGgQAAADAC/zxxx9q3bq1ypYtq08++UTff/+9Ro8erQoVKhjHjBo1SuPHj9eUKVO0bt06BQYGqnPnzsrPzy/2dViDAAAAAHiBl156SdWrV9f06dONsdjYWOPPdrtd48aN09NPP61u3bpJkt5++21FRkZq8eLF6t27d7GuQ4IAAAAAmLlwipHNZlNubq7DZrPZzlvWkiVL1KJFC916662KiIhQs2bN9MYbbxj79+zZo6ysLMXHxxtjoaGhatWqldasWVPs26dBAAAAANwkNTVVoaGhDltqaup5j929e7cmT56sOnXqaNmyZXr44Yf1yCOPaObMmZKkrKwsSVJkZKTD5yIjI419xcEUIwAAAMDMhY85TUlJUXJyssOY1Wo977GFhYVq0aKFRo4cKUlq1qyZtm7dqilTpigxMbHEaiJBAAAAANzEarUqJCTEYbtQg1ClShU1bNjQYaxBgwbKzMyUJEVFRUmSsrOzHY7Jzs429hUHDQIAAABg5qGPOW3durW2b9/uMPbTTz8pJiZG0ukFy1FRUVqxYoWxPzc3V+vWrVNcXFyxr8MUIwAAAMALDBw4UFdffbVGjhyp2267Td98842mTp2qqVOnSpIsFosGDBig559/XnXq1FFsbKyeeeYZVa1aVd27dy/2dWgQAAAAADMXrkFwRsuWLbVo0SKlpKRoxIgRio2N1bhx43TnnXcaxzz22GPKy8vTAw88oCNHjuiaa67Rp59+Kn9//2Jfx2K32+2lcQPuFHX/QneXAHil9S91dXcJgFeqHHL++cIALszfg39NHdB9qsuudWLxAy67VnF58L8aAAAAwA2cXBvwX+Pbdw8AAADAAQkCAAAAYOahaxBchQQBAAAAgIEEAQAAADCxkCAAAAAAwGkkCAAAAIAJCQIAAAAAFCFBAAAAAMx8O0AgQQAAAABwFg0CAAAAAANTjAAAAAATFikDAAAAQBESBAAAAMCEBAEAAAAAipAgAAAAACYkCAAAAABQhAQBAAAAMCFBAAAAAIAiJAgocYO7NtTgmxs6jO3Yl6s2z352zrGzH7lG1zaOUp+JX+vTTXtdVSLgNY7n5WnG1AlavSpNRw4fVu269fV/Ax9X/YaN3F0a4LGmvfG6Viz/THv27JbV319NmzbTgOTBqhFb092lwVv4doBAg4DS8ePvObp1zCrj64JC+znHPBBfR3adOw7grNGpw/Tz7p164tkXVLFShD5f9qEee+QBvTV7kSpFRLq7PMAjbVj/jXrdfqcua9xYBX8W6LVXx+ih++/V+0s+Uvny5d1dHuDxmGKEUvFnoV0Hcm3GdvjYSYf9l1UP1UOd6mjAjA1uqhDwfLb8fH258nPdnzRQlzdroUurRyvxvv/TpdWqa8mi+e4uD/BYk6dOU7dbElS7dh3Vq19fI154Ufv27dUP329zd2nwEhaLxWWbJyJBQKmoGRGkTS/fKNupAm3YfVgj39+i3w+fkCQFlCujyfe1Usq73+pArs3NlQKeq6CgQIUFBSpXrpzDeDmrv7Z+962bqgK8z7GjRyVJIaGhbq4E8A5ubRAOHjyot956S2vWrFFWVpYkKSoqSldffbX69OmjypUru7M8XKSNew7r0enrtTPrmCLD/DXopob64LH2ajd0ufJsf2r4bU20ftchLftun7tLBTxa+cBANWzURO9Mn6roGjVVIbyivlj+iX7Y+p2qVqvu7vIAr1BYWKhRL41U02ZXqE6duu4uB17CU3+z7ypuaxDWr1+vzp07q3z58oqPj1fduqd/aLOzszV+/Hi9+OKLWrZsmVq0aPG357HZbLLZHH8LbS84JUuZsqVWO/5e2tYs488//J6jjbsPa8OLN+jmltV06KhN19SvrPjnPndjhYD3eGLoSL3ywrPqfXO8/MqUUZ26DdShYxft+PF7d5cGeIWRzw/Xrh07NGPWbHeXAngNtzUI/fv316233qopU6ac06XZ7XY99NBD6t+/v9asWfO350lNTdXw4cMdxgKb3aqg5reVeM24OLknTmn3/qOKrRykBpeGqkblIP30ajeHY6Y9HKd1Ow4q4ZV0N1UJeKaq1aprzOTpOnHiuI7n5alipcp67ukhirq0mrtLAzzeyOdHaFX6Sr018x1FRkW5uxx4ERIEN/nuu+80Y8aM8/4LsFgsGjhwoJo1a/aP50lJSVFycrLDWJ0BH5VYnfj3ylvLKKZykLJzMrVkw6+a/eUeh/0rh3fSs/O+0/LNPOYUuJCAgPIKCCivo7m52rDua92fNNDdJQEey263K/WF55S2YrmmzZilakzJA5zitgYhKipK33zzjerXr3/e/d98840iI//5EX5Wq1VWq9VhjOlF7jW05+X6bPNe/XbouCLDAjTk5oYqLLRr8TeZOnTs5HkXJv9++LgyDx53Q7WAZ1u/9ivZ7XZVj6mhvb/9qqkTxqh6TA1df1O3f/4w4KNGPjdcn3z8oca9NkmB5QN18MABSVJQcLD8/f3dXB28AQmCmwwePFgPPPCAMjIydN111xnNQHZ2tlasWKE33nhDr7zyirvKw79QpUKAJt/fShUCy+nQMZu+2XFIN6Sm6dBfHnUK4J/lHTumaVNe1cH92QoOCVWb9vHq+1B/XXIJvwgBLmT+vDmSpHv73OUwPuL5VHW7JcEdJQFexWK32932pqp58+Zp7NixysjIUEFBgSSpTJkyat68uZKTk3XbbRe3jiDq/oUlWSbgM9a/1NXdJQBeqXKI9Z8PAuDA34Mftl8xcY7LrnVo5u0uu1ZxufVfTa9evdSrVy+dOnVKBw8elCRVqlRJZcvymzEAAADAHTyidytbtqyqVKni7jIAAAAAn+cRDQIAAADgKXx9kbKfuwsAAAAA4DlIEAAAAAATEgQAAAAAKEKCAAAAAJiQIAAAAABAERIEAAAAwMy3AwQSBAAAAABnkSAAAAAAJqxBAAAAAIAiJAgAAACACQkCAAAAABQhQQAAAABMSBAAAAAAoAgJAgAAAGBCggAAAAAARUgQAAAAADPfDhBIEAAAAACcRYMAAAAAwMAUIwAAAMCERcoAAAAAUIQEAQAAADAhQQAAAACAIiQIAAAAgAkJAgAAAAAUIUEAAAAAzHw7QCBBAAAAAHAWCQIAAABgwhoEAAAAAChCggAAAACYkCAAAAAAQBESBAAAAMCEBAEAAAAAipAgAAAAACYkCAAAAABQhAQBAAAAMPPtAIEEAQAAAMBZJAgAAACACWsQAAAAAKAIDQIAAAAAAw0CAAAAYGKxWFy2OWPYsGHnfL5+/frG/vz8fCUlJalixYoKCgpSjx49lJ2d7fT90yAAAAAAXuKyyy7Tvn37jG316tXGvoEDB2rp0qVasGCB0tPTtXfvXiUkJDh9DRYpAwAAACaevEb5kksuUVRU1DnjOTk5mjZtmmbPnq1rr71WkjR9+nQ1aNBAa9eu1VVXXVXsa5AgAAAAAG5is9mUm5vrsNlstgsev2PHDlWtWlU1a9bUnXfeqczMTElSRkaGTp06pfj4eOPY+vXrKzo6WmvWrHGqJhoEAAAAwMSVaxBSU1MVGhrqsKWmpp63rlatWmnGjBn69NNPNXnyZO3Zs0dt2rTR0aNHlZWVpXLlyiksLMzhM5GRkcrKynLq/pliBAAAALhJSkqKkpOTHcasVut5j+3SpYvx58svv1ytWrVSTEyM5s+fr4CAgBKriQYBAAAAMHHlGgSr1XrBhuCfhIWFqW7dutq5c6c6duyokydP6siRIw4pQnZ29nnXLPwdphgBAAAAXujYsWPatWuXqlSpoubNm6ts2bJasWKFsX/79u3KzMxUXFycU+clQQAAAABMnH0/gasMHjxYXbt2VUxMjPbu3auhQ4eqTJkyuv322xUaGqp7771XycnJCg8PV0hIiPr376+4uDinnmAk0SAAAAAAXuG3337T7bffrkOHDqly5cq65pprtHbtWlWuXFmSNHbsWPn5+alHjx6y2Wzq3LmzJk2a5PR1aBAAAAAAEw8NEDR37ty/3e/v76+JEydq4sSJ/+o6rEEAAAAAYCBBAAAAAEz8/Dw0QnAREgQAAAAABhIEAAAAwMRT1yC4CgkCAAAAAAMJAgAAAGDiqe9BcBUSBAAAAAAGGgQAAAAABqYYAQAAACY+PsOIBAEAAADAWSQIAAAAgAmLlAEAAACgCAkCAAAAYEKCAAAAAABFSBAAAAAAEx8PEEgQAAAAAJxFggAAAACYsAYBAAAAAIqQIAAAAAAmPh4gkCAAAAAAOIsEAQAAADBhDQIAAAAAFCFBAAAAAEx8PEAgQQAAAABwFgkCAAAAYMIaBAAAAAAoQoIAAAAAmPh4gECCAAAAAOAsGgQAAAAABqYYAQAAACYsUgYAAACAIv/JBGHHawnuLgHwShFxj7i7BMArHVr3mrtLALyQ5/6W3scDBBIEAAAAAGf9JxMEAAAA4GKxBgEAAAAAipAgAAAAACY+HiCQIAAAAAA4iwQBAAAAMGENAgAAAAAUIUEAAAAATHw8QCBBAAAAAHAWCQIAAABgwhoEAAAAAChCggAAAACYkCAAAAAAQBESBAAAAMDExwMEEgQAAAAAZ9EgAAAAADAwxQgAAAAwYZEyAAAAABQhQQAAAABMfDxAIEEAAAAAcBYJAgAAAGDCGgQAAAAAKEKCAAAAAJj4eIBAggAAAADgLBIEAAAAwMTPxyMEEgQAAAAABhIEAAAAwMTHAwQSBAAAAABnkSAAAAAAJrwHAQAAAACKkCAAAAAAJn6+HSCQIAAAAAA4iwQBAAAAMGENAgAAAAAUIUEAAAAATHw8QCBBAAAAAHAWDQIAAAAAA1OMAAAAABOLfHuOEQkCAAAAAAMJAgAAAGDCi9IAAAAAeJUXX3xRFotFAwYMMMby8/OVlJSkihUrKigoSD169FB2drbT56ZBAAAAAEwsFovLtouxfv16vf7667r88ssdxgcOHKilS5dqwYIFSk9P1969e5WQkOD0+WkQAAAAAC9x7Ngx3XnnnXrjjTdUoUIFYzwnJ0fTpk3TmDFjdO2116p58+aaPn26vv76a61du9apa9AgAAAAACYWi+s2m82m3Nxch81ms12wtqSkJN14442Kj493GM/IyNCpU6ccxuvXr6/o6GitWbPGqfunQQAAAADcJDU1VaGhoQ5bamrqeY+dO3euNm7ceN79WVlZKleunMLCwhzGIyMjlZWV5VRNPMUIAAAAMPG7yLUBFyMlJUXJyckOY1ar9Zzjfv31Vz366KNavny5/P39S7UmGgQAAADATaxW63kbgr/KyMjQ/v37dcUVVxhjBQUFWrVqlSZMmKBly5bp5MmTOnLkiEOKkJ2draioKKdqokEAAAAATFwYIBTbddddpy1btjiM9e3bV/Xr19fjjz+u6tWrq2zZslqxYoV69OghSdq+fbsyMzMVFxfn1LVoEAAAAAAPFxwcrEaNGjmMBQYGqmLFisb4vffeq+TkZIWHhyskJET9+/dXXFycrrrqKqeuRYMAAAAAmFzs+wncbezYsfLz81OPHj1ks9nUuXNnTZo0yenz0CAAAAAAXmjlypUOX/v7+2vixImaOHHivzovDQIAAABg4qUBQonhPQgAAAAADCQIAAAAgIkr34PgiUgQAAAAABhoEAAAAAAYmGIEAAAAmPj2BCMSBAAAAAAmJAgAAACAibe+KK2kkCAAAAAAMJAgAAAAACZ+vh0gkCAAAAAAOIsEAQAAADBhDQIAAAAAFCFBAAAAAEx8PEAgQQAAAABwFgkCAAAAYMIaBAAAAAAoQoIAAAAAmPAeBAAAAAAoQoIAAAAAmPj6GoRiNQhLliwp9glvvvnmiy4GAAAAgHsVq0Ho3r17sU5msVhUUFDwb+oBAAAA3Mq384NiNgiFhYWlXQcAAAAAD8AaBAAAAMDEjzUIzsvLy1N6eroyMzN18uRJh32PPPJIiRQGAAAAwPWcbhC+/fZb3XDDDTp+/Ljy8vIUHh6ugwcPqnz58oqIiKBBAAAAALyY0+9BGDhwoLp27ao//vhDAQEBWrt2rX755Rc1b95cr7zySmnUCAAAALiMxeK6zRM53SBs2rRJgwYNkp+fn8qUKSObzabq1atr1KhRevLJJ0ujRgAAAAAu4nSDULZsWfn5nf5YRESEMjMzJUmhoaH69ddfS7Y6AAAAwMUsFovLNk/k9BqEZs2aaf369apTp47atWunZ599VgcPHtSsWbPUqFGj0qgRAAAAgIs4nSCMHDlSVapUkSS98MILqlChgh5++GEdOHBAU6dOLfECAQAAAFfy9TUITicILVq0MP4cERGhTz/9tEQLAgAAAOA+vCgNAAAAMOFFaU6KjY392wUVu3fv/lcF4b9vxrQ3NGH8GN1+510a9BhPvgLOqFo5VM8/2k2dWl+m8v5ltevXg3pw2Dva+P3ph0Gc+HbCeT/35NhFGvv2CleWCni0+fPmaOG8Odq793dJUs1atfXAQ0m6pk1bN1cGeAenG4QBAwY4fH3q1Cl9++23+vTTTzVkyJCSqgv/Udu2btH7C+epTt167i4F8ChhwQFKm5Gs9PU71L3fJB3445hqR1fWH7nHjWNqxKc4fKZT68s0ZegdWrRik4urBTxbZGSk+g8YpOiYGMlu19IlizXwkSTNXfC+atWu4+7y4AV8PEBwvkF49NFHzzs+ceJEbdiw4V8XhP+u48fz9EzKED01dISmvTHF3eUAHmVQ3476LesPPTjsHWPsl72HHI7JPnTU4euu7Rsrff0O/fy743GAr2vX/lqHr/s9MlAL5s3V5s3f0SAAxeD0U4wupEuXLnrvvfdK6nT4D3pp5HNq3badWl11tbtLATzOje0aa+P3mXp31D36ZUWq1sx5XH1vufDPSkR4sK6/ppFmLl7jwioB71NQUKBPP/lIJ04c1+VNmrq7HHgJ3oNQQhYuXKjw8PCSOp0k6ddff9XQoUP11ltvXfAYm80mm83mMHbSXlZWq7VEa8G/s+yTj/TjD9/r7dkL3F0K4JFiL62k+29to/HvpGnUtM/U/LIYjX6sp07+WaB3l6475/j/dW2lo8fztThtk+uLBbzAjp+2K/F/t+vkSZsCypfX6HETVKtWbXeXBXiFi3pRmrnbsdvtysrK0oEDBzRp0qQSLe7w4cOaOXPm3zYIqampGj58uMPYE089qyefHlqiteDiZWXt0+hRqZr4+jQaN+AC/Pws2vh9poZOWCpJ+m77b7qsdhXd3/Oa8zYId3e7SvM+2SDbyT9dXSrgFWrExmruwkU6dvSoPl++TM8+/YTenD6LJgHFUmJTbLyU0w1Ct27dHBoEPz8/Va5cWe3bt1f9+vWdOteSJUv+dn9xnoiUkpKi5ORkh7GT9rJO1YHS9eP323T48CH9r3cPY6ygoEDfZmzQ/Lmz9fX671SmTBk3Vgi4X9bBXP2wO8th7Mc9Wep+XdNzjm3drJbqxUbpriemu6g6wPuULVtO0dExkqSGlzXStq1bNeedt/X00BFurgzwfE43CMOGDSuxi3fv3l0Wi0V2u/2Cx/zT3Cyr1XrOb6WP5heWSH0oGS1bxWnuwg8cxkYMfUoxNWKV2Pc+mgNA0ppNu1U3JsJhrE50hDL3HT7n2MTuccr4PlNbfvrdVeUBXs9uL9TJkyfdXQa8hKeuDXAVpxOUMmXKaP/+/eeMHzp0yOm/6FWpUkXvv/++CgsLz7tt3LjR2fLggQIDA1W7Tl2HzT8gQGFhYapdp667ywM8wmvvpOnKxrEack8n1axeSb2ub6F7erTW6/NWORwXHOivhI7NNGPR126qFPB848eNVsaG9dr7+2/a8dN2jR83WhvWf6Mbbuzq7tIAr+B0gnCh3/bbbDaVK1fOqXM1b95cGRkZ6tat23n3/1O6AAD/FRnfZ6rXoDc0ov/NevKBLvr590Ma8vJ7mvuJ4+Ojb+3cXBZZNP9THisNXMjhw4f1zFOP6+CBAwoKDladOvU0acqbuurq1u4uDV7Cz7cDhOI3COPHj5d0+i/tb775poKCgox9BQUFWrVqldNrEIYMGaK8vLwL7q9du7a++OILp84J7zB12tvuLgHwOJ98uVWffLn1b4956/2v9Nb7X7moIsA7DRvxgrtLALxasRuEsWPHSjqdIEyZMsVhOlG5cuVUo0YNTZni3Muv2rRp87f7AwMD1a5dO6fOCQAAAODiFbtB2LNnjySpQ4cOev/991WhQoVSKwoAAABwF6YYOYkpPwAAAMB/l9NPMerRo4deeumlc8ZHjRqlW2+9tUSKAgAAANzFYrG4bPNETjcIq1at0g033HDOeJcuXbRq1arzfAIAAACAt3B6itGxY8fO+zjTsmXLKjc3t0SKAgAAANzF19cgOJ0gNG7cWPPmzTtnfO7cuWrYsGGJFAUAAADAPZxOEJ555hklJCRo165duvbaayVJK1as0OzZs7Vw4cISLxAAAABwJQ9dGuAyTjcIXbt21eLFizVy5EgtXLhQAQEBatKkidLS0hQeHl4aNQIAAABwEacbBEm68cYbdeONN0qScnNzNWfOHA0ePFgZGRkqKCgo0QIBAAAAV/Lz8QjB6TUIZ6xatUqJiYmqWrWqRo8erWuvvVZr164tydoAAAAAuJhTCUJWVpZmzJihadOmKTc3V7fddptsNpsWL17MAmUAAAD8J1z0b9D/I4p9/127dlW9evW0efNmjRs3Tnv37tVrr71WmrUBAAAAcLFiJwiffPKJHnnkET388MOqU6dOadYEAAAAuI2PL0EofoKwevVqHT16VM2bN1erVq00YcIEHTx4sDRrAwAAAOBixW4QrrrqKr3xxhvat2+fHnzwQc2dO1dVq1ZVYWGhli9frqNHj5ZmnQAAAIBL+FksLts8kdNrMAIDA3XPPfdo9erV2rJliwYNGqQXX3xRERERuvnmm0ujRgAAAAAu8q8WaderV0+jRo3Sb7/9pjlz5pRUTQAAAIDbWCyu2zxRiTzFqUyZMurevbuWLFlSEqcDAAAA4CYX9SZlAAAA4L/Kz0N/s+8qvv4eCAAAAAAmNAgAAAAADEwxAgAAAEw89fGjrkKCAAAAAMBAggAAAACY+HiAQIIAAAAA4CwSBAAAAMCEx5wCAAAAQBEaBAAAAMDE4sJ/nDF58mRdfvnlCgkJUUhIiOLi4vTJJ58Y+/Pz85WUlKSKFSsqKChIPXr0UHZ2ttP3T4MAAAAAeIFq1arpxRdfVEZGhjZs2KBrr71W3bp107Zt2yRJAwcO1NKlS7VgwQKlp6dr7969SkhIcPo6Frvdbi/p4t3taH6hu0sAvFJE3CPuLgHwSofWvebuEgCvU76c5070fzFtl8uu9cS1tf7V58PDw/Xyyy+rZ8+eqly5smbPnq2ePXtKkn788Uc1aNBAa9as0VVXXVXsc5IgAAAAAG5is9mUm5vrsNlstn/8XEFBgebOnau8vDzFxcUpIyNDp06dUnx8vHFM/fr1FR0drTVr1jhVEw0CAAAAYOJncd2Wmpqq0NBQhy01NfWCtW3ZskVBQUGyWq166KGHtGjRIjVs2FBZWVkqV66cwsLCHI6PjIxUVlaWU/fPY04BAAAAN0lJSVFycrLDmNVqveDx9erV06ZNm5STk6OFCxcqMTFR6enpJVoTDQIAAABgYnHhq5StVuvfNgR/Va5cOdWuXVuS1Lx5c61fv16vvvqqevXqpZMnT+rIkSMOKUJ2draioqKcqokpRgAAAICXKiwslM1mU/PmzVW2bFmtWLHC2Ld9+3ZlZmYqLi7OqXOSIAAAAAAmnvom5ZSUFHXp0kXR0dE6evSoZs+erZUrV2rZsmUKDQ3Vvffeq+TkZIWHhyskJET9+/dXXFycU08wkmgQAAAAAK+wf/9+3X333dq3b59CQ0N1+eWXa9myZerYsaMkaezYsfLz81OPHj1ks9nUuXNnTZo0yenr8B4EAAbegwBcHN6DADjPk9+DMGbVbpddK7ltTZddq7hYgwAAAADAQIMAAAAAwMAaBAAAAMDEz4WPOfVEJAgAAAAADCQIAAAAgImnPubUVUgQAAAAABhIEAAAAAATH1+CQIIAAAAA4CwSBAAAAMDET74dIZAgAAAAADCQIAAAAAAmrEEAAAAAgCIkCAAAAIAJ70EAAAAAgCIkCAAAAICJn48vQiBBAAAAAGAgQQAAAABMfDxAIEEAAAAAcBYJAgAAAGDCGgQAAAAAKEKCAAAAAJj4eIBAggAAAADgLBoEAAAAAAamGAEAAAAmvv4bdF+/fwAAAAAmJAgAAACAicXHVymTIAAAAAAwkCAAAAAAJr6dH5AgAAAAADAhQQAAAABM/FiDAAAAAACnkSAAAAAAJr6dH5AgAAAAADAhQQAAAABMfHwJAgkCAAAAgLNIEAAAAAAT3qQMAAAAAEVIEAAAAAATX/8Nuq/fPwAAAAATEgQAAADAhDUIAAAAAFCEBgEAAACAgSlGAAAAgIlvTzAiQQAAAABgQoIAAAAAmPj6IuX/ZINQ9hKCEeBi/PrlOHeXAHilavfOcXcJgNc5POsOd5eAC/hPNggAAADAxfL1XzX7+v0DAAAAMCFBAAAAAEx8fQ0CCQIAAAAAAwkCAAAAYOLb+QEJAgAAAAATEgQAAADAxMeXIJAgAAAAADiLBAEAAAAw8fPxVQgkCAAAAAAMJAgAAACACWsQAAAAAKAICQIAAABgYmENAgAAAACcRoIAAAAAmLAGAQAAAACK0CAAAAAAMDDFCAAAADDhRWkAAAAAUIQEAQAAADBhkTIAAAAAFCFBAAAAAExIEAAAAACgCAkCAAAAYGLhKUYAAAAAcBoJAgAAAGDi59sBAgkCAAAA4A1SU1PVsmVLBQcHKyIiQt27d9f27dsdjsnPz1dSUpIqVqyooKAg9ejRQ9nZ2U5dhwYBAAAAMLG48B9npKenKykpSWvXrtXy5ct16tQpderUSXl5ecYxAwcO1NKlS7VgwQKlp6dr7969SkhIcO7+7Xa73alPeIH8P91dAeCdjvHDA1yUug/Pd3cJgNc5POsOd5dwQWk/HnLZtVrHBslmszmMWa1WWa3Wf/zsgQMHFBERofT0dLVt21Y5OTmqXLmyZs+erZ49e0qSfvzxRzVo0EBr1qzRVVddVayaSBAAAAAAE4vFdVtqaqpCQ0MdttTU1GLVmZOTI0kKDw+XJGVkZOjUqVOKj483jqlfv76io6O1Zs2aYt8/i5QBAAAAN0lJSVFycrLDWHHSg8LCQg0YMECtW7dWo0aNJElZWVkqV66cwsLCHI6NjIxUVlZWsWuiQQAAAABMXPkehOJOJ/qrpKQkbd26VatXry7xmphiBAAAAHiRfv366cMPP9QXX3yhatWqGeNRUVE6efKkjhw54nB8dna2oqKiin1+GgQAAADAxM/ius0Zdrtd/fr106JFi5SWlqbY2FiH/c2bN1fZsmW1YsUKY2z79u3KzMxUXFxcsa/DFCMAAADACyQlJWn27Nn64IMPFBwcbKwrCA0NVUBAgEJDQ3XvvfcqOTlZ4eHhCgkJUf/+/RUXF1fsJxhJNAgAAACAV5g8ebIkqX379g7j06dPV58+fSRJY8eOlZ+fn3r06CGbzabOnTtr0qRJTl2HBgEAAAAwceUiZWcU5/Vl/v7+mjhxoiZOnHjR12ENAgAAAAADCQIAAABgYvHMAMFlSBAAAAAAGEgQAAAAABMfDxBIEAAAAACcRYIAAAAAmPj5+CIEEgQAAAAABhIEAAAAwMS38wMSBAAAAAAmJAgAAACAmY9HCCQIAAAAAAwkCAAAAICJxccjBBIEAAAAAAYSBAAAAMDEx1+DQIIAAAAA4CwSBAAAAMDExwMEEgQAAAAAZ5EgAAAAAGY+HiGQIAAAAAAw0CAAAAAAMDDFCAAAADDhRWkAAAAAUIQEAQAAADDhRWkAAAAAUIQEAQAAADDx8QCBBAEAAADAWSQIAAAAgJmPRwgkCAAAAAAMJAgAAACACe9BAAAAAIAiJAgAAACACe9BAAAAAIAiJAgAAACAiY8HCCQIAAAAAM4iQQAAAADMfDxCIEEAAAAAYCBBAAAAAEx4DwIAAAAAFKFBAAAAAGBgihEAAABgwovSAAAAAKAICQIAAABg4uMBAgkCAAAAgLNIEAAAAAAzH48QSBAAAAAAGEgQAAAAABNff1EaDQJK3bQ3XteK5Z9pz57dsvr7q2nTZhqQPFg1Ymu6uzTA4x3Yn61J48do7ddfKj8/X9WqRevJYc+rQcNG7i4N8AiP39JYjyc0dhj7aW+Ornr8I+PrlrUr6albL1fzWpVUWGjXll/+UM9RXyj/VIGrywW8Ag0CSt2G9d+o1+136rLGjVXwZ4Fee3WMHrr/Xr2/5COVL1/e3eUBHis3N0cP3fM/XdHiSo0eP0VhFcL1a+YvCg4OcXdpgEf54bcjuuXFNOPrPwvsxp9b1q6kBUPaa+zS7/XE2xn6s6BQjaIrqNBuP9+pAEm8B4EGAaVu8tRpDl+PeOFFdWgTpx++36bmLVq6qSrA8707Y5oiIqP01LAXjLGql1ZzY0WAZ/qzwK79Ofnn3ffCnVdo6mc/6dUPvzfGdmYddVVpgFeiQYDLHTt6+j/MIaGhbq4E8GyrV32hK+Na6+nHBurbjRtUOSJCCT176+aEW91dGuBRakYFa9v47rKdKtT6nQc1Yv4m/X7ouCqFWNWidiUt+PpnffpsR9WICNKOfbl6fsFmrfvpgLvLhgfz8QCBpxjBtQoLCzXqpZFq2uwK1alT193lAB5t7++/afHCeaoWHaOxE6bqlp69NPaVVH28dLG7SwM8Rsaug+o3dY1ufXmlBs9Yr5jKgfr46Y4K8r9ENSoHSTq9TuHtL3bp1pdXavPPf2jxE9eqZmSwmysHPJfbE4QTJ04oIyND4eHhatiwocO+/Px8zZ8/X3ffffcFP2+z2WSz2RzG7GWsslqtpVIv/p2Rzw/Xrh07NGPWbHeXAni8wsJC1W/YSA/1GyBJqlu/gXbv3KnF783XDV27u7U2wFN8vnmf8efvfz2iDbsOavPYbureKlo/7c2VJM34Yqdmf7lbkrTllz/UtmGk7mxXU8/N/84tNcML+HiE4NYE4aefflKDBg3Utm1bNW7cWO3atdO+fWd/0HNyctS3b9+/PUdqaqpCQ0MdtpdfSi3t0nERRj4/QqvSV+qN6TMVGRXl7nIAj1exUmXViK3lMFYjtqays/Zd4BMAco+f0s6so4qNDFbWkROSpO2/5zgc89PeXFWrGOiO8gCv4NYG4fHHH1ejRo20f/9+bd++XcHBwWrdurUyMzOLfY6UlBTl5OQ4bEMeTynFquEsu92ukc+PUNqK5XrjrZmqVq26u0sCvMLlTZop85c9DmOZmT8rqkpVN1UEeL5A6yWKjQhS9pETyjyQp72Hj6tOFccnf9WKCtavB/PcVCG8gcWF/3gitzYIX3/9tVJTU1WpUiXVrl1bS5cuVefOndWmTRvt3r27WOewWq0KCQlx2Jhe5FlGPjdcH3+4RC+OGq3A8oE6eOCADh44oPz88z9xAsBpve68W9u2bNbMt6bqt19/0WeffKgl7y9Uwq23u7s0wGOMuL2Zrq4foeqVAnVlnUqaNaCNCgrtem/NL5KkCR//oAc61dXNLasrNiJIT/a4XHWqhuid9F1urhzwXBa73X0PAg4JCdG6devUoEEDh/F+/frpgw8+0OzZs9W+fXsVFDj3IpP8P0uySvxbTS6rd97xEc+nqtstCS6uBn/nGD88HuerVSs1ZcI4/fbrL6pStZp633k3TzHyQHUfnu/uEnzWm0mtFVevssKDrDp01Ka1Px3Q8wu+08/7jxnHPHpTQ90XX0dhQVZty/xDQ+du4ilGHuDwrDvcXcIFbc867rJr1YvyvHdCubVBuPLKK9W/f3/ddddd5+zr16+f3n33XeXm5tIgAC5CgwBcHBoEwHk0CKd5YoPg1ilGt9xyi+bMmXPefRMmTNDtt98uN/YvAAAAgM9xa4JQWvglKHBxSBCAi0OCADjPkxOEn1yYINQlQQAAAADgydz+ojQAAADAo3jm00ddhgQBAAAAgIEEAQAAADDx1BeYuQoJAgAAAAADCQIAAABgYvHtAIEEAQAAAMBZJAgAAACAiY8HCCQIAAAAAM4iQQAAAADMfDxCIEEAAAAAYCBBAAAAAEx4DwIAAAAAj7dq1Sp17dpVVatWlcVi0eLFix322+12Pfvss6pSpYoCAgIUHx+vHTt2OH0dGgQAAADAxGJx3eaMvLw8NWnSRBMnTjzv/lGjRmn8+PGaMmWK1q1bp8DAQHXu3Fn5+flOXYcpRgAAAIAX6NKli7p06XLefXa7XePGjdPTTz+tbt26SZLefvttRUZGavHixerdu3exr0OCAAAAAJhYXLjZbDbl5uY6bDabzema9+zZo6ysLMXHxxtjoaGhatWqldasWePUuWgQAAAAADdJTU1VaGiow5aamur0ebKysiRJkZGRDuORkZHGvuJiihEAAABg5sKHGKWkpCg5OdlhzGq1uq6A86BBAAAAANzEarWWSEMQFRUlScrOzlaVKlWM8ezsbDVt2tSpczHFCAAAAPBysbGxioqK0ooVK4yx3NxcrVu3TnFxcU6diwQBAAAAMPHUF6UdO3ZMO3fuNL7es2ePNm3apPDwcEVHR2vAgAF6/vnnVadOHcXGxuqZZ55R1apV1b17d6euQ4MAAAAAeIENGzaoQ4cOxtdn1i4kJiZqxowZeuyxx5SXl6cHHnhAR44c0TXXXKNPP/1U/v7+Tl3HYrfb7SVauQfI/9PdFQDe6Rg/PMBFqfvwfHeXAHidw7PucHcJF5R52PnHjF6s6HD3Lkg+H9YgAAAAADAwxQgAAAAw8cwVCK5DggAAAADAQIIAAAAAmFh8PEIgQQAAAABgIEEAAAAAHPh2hECCAAAAAMBAggAAAACYsAYBAAAAAIqQIAAAAAAmPh4gkCAAAAAAOIsEAQAAADBhDQIAAAAAFCFBAAAAAEwsPr4KgQQBAAAAgIEGAQAAAICBKUYAAACAmW/PMCJBAAAAAHAWCQIAAABg4uMBAgkCAAAAgLNIEAAAAAATXpQGAAAAAEVIEAAAAAATXpQGAAAAAEVIEAAAAAAz3w4QSBAAAAAAnEWCAAAAAJj4eIBAggAAAADgLBIEAAAAwIT3IAAAAABAERIEAAAAwIT3IAAAAABAERIEAAAAwIQ1CAAAAABQhAYBAAAAgIEGAQAAAICBBgEAAACAgUXKAAAAgAmLlAEAAACgCAkCAAAAYMKL0gAAAACgCAkCAAAAYMIaBAAAAAAoQoIAAAAAmPh4gECCAAAAAOAsEgQAAADAzMcjBBIEAAAAAAYSBAAAAMCE9yAAAAAAQBESBAAAAMCE9yAAAAAAQBESBAAAAMDExwMEEgQAAAAAZ5EgAAAAAGY+HiGQIAAAAAAw0CAAAAAAMDDFCAAAADDhRWkAAAAAUIQEAQAAADDhRWkAAAAAUMRit9vt7i4CvsNmsyk1NVUpKSmyWq3uLgfwCvzcABeHnx3g4tAgwKVyc3MVGhqqnJwchYSEuLscwCvwcwNcHH52gIvDFCMAAAAABhoEAAAAAAYaBAAAAAAGGgS4lNVq1dChQ1ksBjiBnxvg4vCzA1wcFikDAAAAMJAgAAAAADDQIAAAAAAw0CAAAAAAMNAgAAAAADDQIMBlJk6cqBo1asjf31+tWrXSN9984+6SAI+2atUqde3aVVWrVpXFYtHixYvdXRLgFVJTU9WyZUsFBwcrIiJC3bt31/bt291dFuA1aBDgEvPmzVNycrKGDh2qjRs3qkmTJurcubP279/v7tIAj5WXl6cmTZpo4sSJ7i4F8Crp6elKSkrS2rVrtXz5cp06dUqdOnVSXl6eu0sDvAKPOYVLtGrVSi1bttSECRMkSYWFhapevbr69++vJ554ws3VAZ7PYrFo0aJF6t69u7tLAbzOgQMHFBERofT0dLVt29bd5QAejwQBpe7kyZPKyMhQfHy8Mebn56f4+HitWbPGjZUBAHxBTk6OJCk8PNzNlQDegQYBpe7gwYMqKChQZGSkw3hkZKSysrLcVBUAwBcUFhZqwIABat26tRo1auTucgCvcIm7CwAAACgtSUlJ2rp1q1avXu3uUgCvQYOAUlepUiWVKVNG2dnZDuPZ2dmKiopyU1UAgP+6fv366cMPP9SqVatUrVo1d5cDeA2mGKHUlStXTs2bN9eKFSuMscLCQq1YsUJxcXFurAwA8F9kt9vVr18/LVq0SGlpaYqNjXV3SYBXIUGASyQnJysxMVEtWrTQlVdeqXHjxikvL099+/Z1d2mAxzp27Jh27txpfL1nzx5t2rRJ4eHhio6OdmNlgGdLSkrS7Nmz9cEHHyg4ONhY7xYaGqqAgAA3Vwd4Ph5zCpeZMGGCXn75ZWVlZalp06YaP368WrVq5e6yAI+1cuVKdejQ4ZzxxMREzZgxw/UFAV7CYrGcd3z69Onq06ePa4sBvBANAgAAAAADaxAAAAAAGGgQAAAAABhoEAAAAAAYaBAAAAAAGGgQAAAAABhoEAAAAAAYaBAAAAAAGGgQAAAAABhoEADAw/Tp00fdu3c3vm7fvr0GDBjg8jpWrlwpi8WiI0eOuPzaAAD3oUEAgGLq06ePLBaLLBaLypUrp9q1a2vEiBH6888/S/W677//vp577rliHctf6gEA/9Yl7i4AALzJ9ddfr+nTp8tms+njjz9WUlKSypYtq5SUFIfjTp48qXLlypXINcPDw0vkPAAAFAcJAgA4wWq1KioqSjExMXr44YcVHx+vJUuWGNOCXnjhBVWtWlX16tWTJP3666+67bbbFBYWpvDwcHXr1k0///yzcb6CggIlJycrLCxMFStW1GOPPSa73e5wzb9OMbLZbHr88cdVvXp1Wa1W1a5dW9OmTdPPP/+sDh06SJIqVKggi8WiPn36SJIKCwuVmpqq2NhYBQQEqEmTJlq4cKHDdT7++GPVrVtXAQEB6tChg0OdAADfQYMAAP9CQECATp48KUlasWKFtm/fruXLl+vDDz/UqVOn1LlzZwUHB+vLL7/UV199paCgIF1//fXGZ0aPHq0ZM2borbfe0urVq3X48GEtWrTob6959913a86cORo/frx++OEHvf766woKClL16tX13nvvSZK2b9+uffv26dVXX5Ukpaam6u2339aUKVO0bds2DRw4UP/73/+Unp4u6XQjk5CQoK5du2rTpk2677779MQTT5TWtw0A4MGYYgQAF8Fut2vFihVatmyZ+vfvrwMHDigwMFBvvvmmMbXonXfeUWFhod58801ZLBZJ0vTp0xUWFqaVK1eqU6dOGjdunFJSUpSQkCBJmjJlipYtW3bB6/7000+aP3++li9frvj4eElSzZo1jf1npiNFREQoLCxM0unEYeTIkfr8888VFxdnfGb16tV6/fXX1a5dO02ePFm1atXS6NGjJUn16tXTli1b9NJLL5Xgdw0A4A1oEADACR9++KGCgoJ06tQpFRYW6o477tCwYcOUlJSkxo0bO6w7+O6777Rz504FBwc7nCM/P1+7du1STk6O9u3bp1atWhn7LrnkErVo0eKcaUZnbNq0SWXKlFG7du2KXfPOnTt1/PhxdezY0WH85MmTatasmSTphx9+cKhDktFMAAB8Cw0CADihQ4cOmjx5ssqVK6eqVavqkkvO/mc0MDDQ4dhjx46pefPmevfdd885T+XKlS/q+gEBAU5/5tixY5Kkjz76SJdeeqnDPqvVelF1AAD+u2gQAMAJgYGBql27drGOveKKKzRv3jxFREQoJCTkvMdUqVJF69atU9u2bSVJf/75pzIyMnTFFVec9/jGjRursLBQ6enpxhQjszMJRkFBgTHWsGFDWa1WZWZmXjB5aNCggZYsWeIwtnbt2n++SQDAfw6LlAGglNx5552qVKmSunXrpi+//FJ79uzRypUr9cgjj+i3336TJD366KN68cUXtXjxYv3444/6v//7v799h0GNGjWUmJioe+65R4sXLzbOOX/+fElSTEyMLBaLPvzwQx04cEDHjh1TcHCwBg8erIEDB2rmzJnatWuXNm7cqNdee00zZ86UJD300EPasWOHhgwZou3bt2v27NmaMWNGaX+LAAAeiAYBAEpJ+fLltWrVKkVHRyshIUENGjTQvffeq/z8fCNRGDRokO666y4lJiYqLi5OwcHBuuWWW/72vJMnT1bPnj31f//3f6pfv77uv/9+5eXlSZIuvfRSDR8+XE888YQiIyPVr18/SdJzzz2nZ555RqmpqWrQoIGuv/56ffTRR4qNjZUkRUdH67333tPixYvVpEkTTZkyRSNHjizF7w4AwFNZ7BdaCQcAAADA55AgAAAAADDQIAAAAAAw0CAAAAAAMNAgAAAAADDQIAAAAAAw0CAAAAAAMNAgAAAAADDQIAAAAAAw0CAAAAAAMNAgAAAAADDQIAAAAAAw/D+TERzljDoniQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe78f67c-e894-4acd-93b1-34552510c6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "â€œPython(gpu)â€",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
